
. Deploy bootstrap node
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  master_count             = 0
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" != "stackit"]
. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----
endif::[]
ifeval::["{provider}" == "stackit"]
. Set up DNS NS records on parent zone using the data from the Terraform output variable ns_records from the previous step
+
[WARNING]
--
`component-openshift4-terraform` doesn't currently support the STACKIT cluster Terraform module, so at this time, the Terraform output for NS records isn't provided.
--
endif::[]
ifeval::["{provider}" == "cloudscale"]
. Store the subnet UUID and ingress floating IP in the cluster configuration
+
[source,bash]
----
export SUBNET_UUID="$(terraform output -raw subnet_uuid)"
export INGRESS_FLOATING_IP="$(terraform output -raw router_vip)"

pushd ../../../inventory/classes/${TENANT_ID}

yq eval -i '.parameters.openshift.cloudscale.subnet_uuid = "'$SUBNET_UUID'"' \
  ${CLUSTER_ID}.yml
yq eval -i '.parameters.openshift.cloudscale.ingress_floating_ip_v4 = "'$INGRESS_FLOATING_IP'"' \
  ${CLUSTER_ID}.yml

git commit -am "Configure cloudscale subnet UUID and ingress floating IP for ${CLUSTER_ID}"
git push
popd
popd # yes, twice.
----

. Compile and push the cluster catalog
+
include::partial$install/commodore-dynfacts.adoc[]

. Return to Terraform directory
+
[source,bash]
----
pushd catalog/manifests/openshift4-terraform/

terraform init \
  "-backend-config=address=${GITLAB_STATE_URL}" \
  "-backend-config=lock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=unlock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=username=${GITLAB_USER}" \
  "-backend-config=password=${GITLAB_TOKEN}" \
  "-backend-config=lock_method=POST" \
  "-backend-config=unlock_method=DELETE" \
  "-backend-config=retry_wait_min=5"
----
endif::[]

. Wait for bootstrap API to come up
+
[source,bash]
----
API_URL=$(yq e '.clusters[0].cluster.server' "${INSTALLER_DIR}/auth/kubeconfig")
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" == "cloudscale"]
. Add the DNS records for etcd shown in output variable `dns_entries` from the previous step to the cluster's parent zone
endif::[]

. Wait for master nodes to become ready
+
TIP: This is optional, but will make the subsequent steps less likely to run into weird timeouts.
+
[source,bash]
----
export KUBECONFIG="${INSTALLER_DIR}/auth/kubeconfig"

+kubectl wait --for create --timeout=600s node -l node-role.kubernetes.io/master
+kubectl wait --for condition=ready --timeout=600s node -l node-role.kubernetes.io/master
----

. Deploy cert-manager
+
[NOTE]
====
We need to deploy cert-manager early so we can use the cert-manager integration in the Cilium Helm chart.
ifeval::["{provider}" == "cloudscale"]

On cloudscale, we additionally need cert-manager in order to deploy the cloudscale-loadbalancer-controller.
endif::[]
====
+
[source,bash]
----
kubectl apply -f ../cert-manager/00_namespace.yaml
kubectl apply -Rf ../cert-manager/10_cert_manager
kubectl -n syn-cert-manager patch --type=merge \
  $(kubectl -n syn-cert-manager get deploy -oname) \
  -p '{"spec":{"template":{"spec":{"tolerations":[{"operator":"Exists"}]}}}}'
----

ifeval::["{provider}" == "cloudscale"]
. Apply the manifests for the cloudscale machine-api provider
+
[source,bash,subs="attributes+"]
----
export VAULT_TOKEN=$(vault token lookup -format=json | jq -r .data.id) 
kapitan refs --reveal --refs-path ../../refs -f ../machine-api-provider-cloudscale/00_secrets.yaml | kubectl apply -f -

kubectl apply  -f ../machine-api-provider-cloudscale/10_clusterRoleBinding.yaml

kubectl apply -f ../machine-api-provider-cloudscale/10_serviceAccount.yaml

kubectl apply -f ../machine-api-provider-cloudscale/11_deployment.yaml
----

. Apply the machinesets from the catalog
+
[source,bash,subs="attributes+"]
----
kubectl apply `for f in ../openshift4-nodes/machineset-*.yaml ; do echo -n "-f $f " ; done`
----

. Deploy the cloudscale loadbalancer controller
+
[source,bash]
----
kubectl apply -f ../cloudscale-loadbalancer-controller/00_namespace.yaml
kapitan refs --reveal --refs-path ../../refs -f ../cloudscale-loadbalancer-controller/10_secrets.yaml | kubectl apply -f -
kubectl apply -Rf ../cloudscale-loadbalancer-controller/10_kustomize
kubectl -n appuio-cloudscale-loadbalancer-controller \
  wait --for condition=available \
  deploy cloudscale-loadbalancer-controller-controller-manager
----

. Deploy the ingress cloudscale loadbalancer
+
[source,bash]
----
kubectl apply -f ../cloudscale-loadbalancer-controller/20_loadbalancers.yaml
----
endif::[]

. Wait for bootstrap to complete
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  wait-for bootstrap-complete --log-level debug
----
+
[NOTE]
--
If you're using a CNI other than Cilium you may need to remove the following taint from the nodes to allow the network to come up:
[source,bash]
----
kubectl taint no --all node.cloudprovider.kubernetes.io/uninitialized:NoSchedule-
----
Once the bootstrap is complete, taint the master nodes again to ensure that they're properly initialized by the cloud-controller-manager.
[source,bash]
----
kubectl taint no -l node-role.kubernetes.io/master node.cloudprovider.kubernetes.io/uninitialized=:NoSchedule
----
--

ifeval::["{provider}" != "cloudscale"]
. Remove bootstrap node and provision remaining nodes
endif::[]
ifeval::["{provider}" == "cloudscale"]
. Remove bootstrap node
endif::[]
+
[source,bash,subs="attributes+"]
----
rm override.tf
terraform apply

popd
----

ifeval::["{provider}" != "stackit"]
. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----
endif::[]

ifeval::["{provider}" != "cloudscale"]
. Approve node certs
+
include::partial$install/approve-node-csrs.adoc[]

. Label infra nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra-")).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/infra=
----

ifeval::["{provider}" == "exoscale"]
. Label and taint storage nodes
+
include::partial$label-taint-storage-nodes.adoc[]
endif::[]

. Label worker nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra|master|storage-")|not).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/app=
----
+
[NOTE]
At this point you may want to add extra labels to the additional worker groups, if there are any.
endif::[]

. Enable proxy protocol on ingress controller
+
[source,bash]
----
kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/endpointPublishingStrategy",
    "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
  }]'
----
+
[TIP]
====
This step isn't necessary if you've disabled the proxy protocol on the load-balancers manually during setup.

By default, PROXY protocol is enabled through the VSHN Commodore global defaults.
====

. Schedule ingress controller on infra nodes
+
NOTE: Skip this step for OKE clusters
+
[source,bash]
----
kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/nodePlacement",
    "value":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra":""}}}
  }]'
----

. Remove temporary tolerations on cert-manager deployments
+
[source,bash]
----
kubectl -n syn-cert-manager patch --type=json \
  $(kubectl -n syn-cert-manager get deploy -oname) \
  -p '[{"op":"remove","path":"/spec/template/spec/tolerations"}]'
----

. Wait for installation to complete
+
[source,bash]
----
openshift-install --dir ${INSTALLER_DIR} \
  wait-for install-complete --log-level debug
----
