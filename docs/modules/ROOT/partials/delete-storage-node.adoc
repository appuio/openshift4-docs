. Drain the node(s) to replace
+
[source,bash,subs="attributes+"]
----
for node in $(echo -n {node-delete-list}); do
  kubectl --as=cluster-admin drain "${node}" \
    --delete-emptydir-data --ignore-daemonsets
done
----
+
ifeval::["{cloud_provider}" == "cloudscale"]
[TIP]
====
On cloudscale.ch, we configure Rook Ceph to setup the OSDs in "portable" mode.
This configuration enables OSDs to be scheduled on any storage node.

With this configuration, we don't have to migrate OSDs hosted on the old node(s) manually. 
Instead, draining a node will cause any OSDs hosted on that node to be rescheduled on other storage nodes.
====
endif::[]

. Delete the node(s) to replace from the cluster
+
[source,bash,subs="attributes+"]
----
for node in $(echo -n {node-delete-list}); do
  kubectl --as=cluster-admin delete node "${node}"
done
----

ifeval::["{delete-nodes-manually}" == "yes"]
ifeval::["{cloud_provider}" == "exoscale"]
. Remove the Exoscale VM(s)
+
[source,bash,subs="attributes+"]
----
for node in $(echo -n {node-delete-list}); do
  node_id=$(exo vm list -O json | \
    jq --arg storage_node "$node" -r \
    '.[] | select(.name==$storage_node) | .id')

  echo "Removing node:"
  exo vm list | grep "${node_id}"

  exo vm delete "${node_id}"
done
----
endif::[]
ifeval::["{cloud_provider}" == "cloudscale"]
. Remove the cloudscale.ch VM(s)
+
[source,bash,subs="attributes+"]
----
for node in $(echo -n {node-delete-list}); do
  node_id=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
    https://api.cloudscale.ch/v1/servers | \
    jq --arg storage_node "$node" -r \
    '.[] | select(.name|startswith($storage_node)) | .uuid')

  echo "Removing node:"
  curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
    "https://api.cloudscale.ch/v1/servers/${node_id}" |\
    jq -r '.name'

  curl -XDELETE -H "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
    "https://api.cloudscale.ch/v1/servers/${node_id}"
done
----
endif::[]
endif::[]

ifeval::["{delete-nodes-manually}" != "yes"]
. Remove the node(s) by applying Terraform
+
[WARNING]
====
Verify that the hostname of the to be deleted node(s) matches `{node-delete-list}`
====
+
NOTE: Ensure that you're still in directory `${WORK_DIR}/catalog/manifests/openshift4-terraform` before executing this command.
+
[source,bash]
----
terraform apply
----

endif::[]

ifeval::["{cloud_provider}" == "exoscale]
. Clean up localstorage PV(s) of decommissioned node(s)
+
[source,bash,subs="attributes+"]
----
for pv_name in $(echo -n ${delete-pvs}); do
  kubectl --as=cluster-admin delete pv "${pv_name}"
done
----
endif::[]
