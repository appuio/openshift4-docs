= Installation on Exoscale

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Cloudscale

== Prerequisites

* Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key]
* `docker`
* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)
* `md5sum`
* `virt-edit`
* `cpio`
* `openshift-install` (direct download: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-linux.tar.gz[linux], https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-mac.tar.gz[macOS])
* Clone of the https://github.com/appuio/terraform-openshift4-exoscale[terraform-openshift4-exoscale] repository
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `curl`
* `gzip`
* A copy of the Ursula Debian package

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

Register the new OpenShift 4 cluster in Lieutenant.

.Lieutenant API endpoint
****
Use the following endpoint for Lieutenant:

VSHN:: https://api.syn.vshn.net
****

=== Set up LDAP service

. Create an LDAP service
+
Use https://control.vshn.net/vshn/services/_create to create a service.
The name must contain the customer and the cluster name.
And then put the LDAP service ID in the following variable:
+
[source,console]
----
export LDAP_ID="Your_LDAP_ID_here"
export LDAP_PASSWORD="Your_LDAP_pw_here"
----

=== Configure input

.Access to various APIs
[source,console]
----
export EXOSCALE_ACCOUNT=<exoscale-account>
export EXOSCALE_API_KEY=<exoscale-key>
export EXOSCALE_API_SECRET=<exoscale-secret>
export EXOSCALE_REGION=<exoscale-zone>

# From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_TOKEN=<gitlab-api-token>

# For example: https://api.syn.vshn.net
# IMPORTANT: do NOT add a trailing `/`. Commands below will fail.
export COMMODORE_API_URL=<lieutenant-api-endpoint>
# See https://wiki.vshn.net/x/ngMBCg#ClusterRegistryinLieutenantSynthesizeaCluster(synfection)-Preparation
export COMMODORE_API_TOKEN=<lieutenant-api-token>
----

.VSHN sepcific input
[source,console]
----
export CLUSTER_ID=<lieutenant-cluster-id> # Looks like: c-<something>
export TENANT_ID=$(curl -sH "Authorization: Bearer ${COMMODORE_API_TOKEN}" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .tenant)
----

.OpenShift configuration
[source,console]
----
export BASE_DOMAIN=<your-base-domain>
export PULL_SECRET='<redhat-pull-secret>' # As copied from https://cloud.redhat.com/openshift/install/pull-secret "Copy pull secret". value must be inside quotes.
----

For `BASE_DOMAIN` explanation, see xref:explanations/dns_scheme.adoc[DNS Scheme].

.Exoscale configuration
[source,console]
----
export AWS_ACCESS_KEY_ID=${EXOSCALE_API_KEY}
export AWS_SECRET_ACCESS_KEY=${EXOSCALE_API_SECRET}
export AWS_S3_ENDPOINT="sos-${EXOSCALE_REGION}.exo.io"

export SSH_PUBLIC_KEY=~/.ssh/id_ed25519.pub
export SSH_PUBKEY_FINGERPRINT=$(ssh-keygen -E md5 -lf $SSH_PUBLIC_KEY | cut -d' ' -f2 | tail -c+5)

# TODO: generate separate restricted IAM key for Ursula
export TF_VAR_lb_exoscale_api_key=<exoscale-key-for-lb>
export TF_VAR_lb_exoscale_api_secret=<exoscale-secret-for-lb>
----

=== Set up S3 bucket

. Prepare Exoscale CLI
+
[source,console]
----
mkdir -p ~/.config/exoscale
cat <<EOF >> ~/.config/exoscale/exoscale.toml

[[accounts]]
  account = "${EXOSCALE_ACCOUNT}"
  defaultZone = "${EXOSCALE_REGION}"
  endpoint = "https://api.exoscale.ch/v1"
  name = "${CLUSTER_ID}"
EOF

----

. Create S3 bucket
+
[source,console]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap"

export TF_VAR_bootstrap_bucket="https://sos-${EXOSCALE_REGION}.exo.io/${CLUSTER_ID}-bootstrap/"
----

=== Upload Red Hat CoreOS image

. Fetch and convert the latest Red Hat CoreOS image
+
[source,console]
----
RHCOS_VERSION="4.7.7"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo vm template register "rhcos-${RHCOS_VERSION}" \
  --checksum $(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }') \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}" \
  --url "https://${AWS_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export TF_VAR_rhcos_template="rhcos-${RHCOS_VERSION}"
----

. Upload Ursula Debian package to bootstrap bucket
+
[source,console]
----
exo storage upload /path/to/ursula.deb "sos://${CLUSTER_ID}-bootstrap/ursula.deb" --acl public-read
----


=== Set secrets in Vault

.Connect with Vault
[source,console]
----
export VAULT_ADDR=https://vault-prod.syn.vshn.net
vault login -method=ldap username=<your.name>
----

.Store various secrets in Vault
[source,console]
----
# Set the exoscale.ch access secrets
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale \
  api_key=${EXOSCALE_API_KEY} \
  api_secret=${EXOSCALE_API_SECRET} \
  s3_access_key=${EXOSCALE_API_KEY} \
  s3_secret_key=${EXOSCALE_API_SECRET}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -
----

=== OpenShift Installer Setup

For the following steps, change into a clean directory (for example a directory in your home).

[CAUTION]
These are the only steps which aren't idempotent and have to be completed uninterrupted in one go.
If you have to recreate the install config or any of the generated manifests you need to rerun all of the subsequent steps.

[NOTE]
--
You can add more options to the `install-config.yaml` file.
Have a look at the https://docs.openshift.com/container-platform/{ocp-minor-version}/installing/installing_bare_metal/installing-bare-metal.html#installation-bare-metal-config-yaml_installing-bare-metal[config example] for more information.

For example, you could change the SDN from a default value to something a customer requests due to some network requirements.
--

. Prepare `install-config.yaml`
+
[source,console]
----
export INSTALLER_DIR="$(pwd)/target"
mkdir -p "${INSTALLER_DIR}"

cat > "${INSTALLER_DIR}/install-config.yaml" <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
sshKey: "$(cat $SSH_PUBLIC_KEY)"
EOF
----

. Render install manifests (this will consume the `install-config.yaml`)
+
[source,console]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create manifests
----

.. If you want to change the default "apps" domain for the cluster:
+
[source,console]
----
yq w -i "${INSTALLER_DIR}/manifests/cluster-ingress-02-config.yml" \
  spec.domain apps.example.com
----

. Prepare install manifests and ignition config
+
[source,console]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create ignition-configs

exo storage upload "${INSTALLER_DIR}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read

export TF_VAR_ignition_ca=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  "${INSTALLER_DIR}/master.ign" | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)
----

=== Terraform Cluster Config

[NOTE]
====
Check https://syn.tools/commodore/running-commodore.html[Running Commodore] for details on how to run commodore.
====

. Prepare Commodore inventory.
+
[source,console]
----
mkdir -p inventory/classes/
git clone $(curl -sH"Authorization: Bearer ${COMMODORE_API_TOKEN}" "${COMMODORE_API_URL}/tenants/${TENANT_ID}" | jq -r '.gitRepo.url') inventory/classes/${TENANT_ID}
----

. Prepare Terraform cluster config
+
[source,console]
----
CA_CERT=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  "${INSTALLER_DIR}/master.ign" | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)

pushd "inventory/classes/${TENANT_ID}/"

yq eval -i '.applications += ["openshift4-terraform"]' ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.infraID = \"$(jq -r .infraID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.clusterID = \"$(jq -r .clusterID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.appsDomain = \"apps.${CLUSTER_ID}.${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.base_domain = \"${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"${CA_CERT}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.rhcos_template = \"${TF_VAR_rhcos_template}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.ssh_key = \"$(cat ${SSH_PUBLIC_KEY})\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.vshnLdap.serviceId = \"${LDAP_ID}\"" \
  ${CLUSTER_ID}.yml


# Have a look at the file ${CLUSTER_ID}.yml.
# Override any default parameters or add more component configuration.

git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push Terraform setup
+
[source,console]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

=== Provision Infrastructure

. Configure Terraform secrets
+
[source,console]
----
cat <<EOF > .env
EXOSCALE_API_KEY
EXOSCALE_API_SECRET
TF_VAR_lb_exoscale_api_key
TF_VAR_lb_exoscale_api_secret
EOF
----

include::partial$setup_terraform_exoscale.adoc[]


. Initialize Terraform
+
[NOTE]
====
Exoscale doesn't allow uploading the same public key multiple times in a single organisation.
If the public key in `$SSH_PUBLIC_KEY` already exists in the organisation, we instruct Terraform to use the existing keypair instead of trying to upload the public key under a new name.
====
+
[source,console]
----
export TF_VAR_cluster_id=$CLUSTER_ID
export TF_VAR_base_domain=${BASE_DOMAIN}
export TF_VAR_ssh_key="$(cut -d' ' -f1,2 <$SSH_PUBLIC_KEY)"
export TF_VAR_existing_keypair=$(exo sshkey list -Ojson | \
        jq -r ".[] | select(.fingerprint == \"${SSH_PUBKEY_FINGERPRINT}\") | .name")
----

. Provision Domain and LoadBalancer
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 0
  master_count    = 0
  infra_count     = 0
  worker_count    = 0
}
EOF
terraform apply
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records` from the previous step

. Deploy bootstrap node
+
[NOTE]
====
We initially provision VMs in "Stopped" state to ensure the private network interface is attached when the VM powers up.
We provision the VMs in this two-step process to avoid VMs getting stuck in a state where they're unable to reach the machine config server when deploying the cluster into a private network.
====
+
[NOTE]
====
We currently need two runs of `terraform apply` with state "Running" when provisioning VMs.
This is necessary because the Exoscale Terraform provider doesn't correctly signal that a VM's IP can change during a state change.
See the issue filed against the https://github.com/exoscale/terraform-provider-exoscale/issues/101[Exoscale Terraform provider] for a more detailed explanation.
====
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  master_count    = 0
  infra_count     = 0
  worker_count    = 0
  bootstrap_state = "${state}"
}
EOF
  terraform apply
done
----

. Wait for bootstrap API to come up
+
[source,console]
----
API_URL=$(yq e '.clusters[0].cluster.server' "${INSTALLER_DIR}/auth/kubeconfig")
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  infra_count     = 0
  worker_count    = 0
  master_state = "${state}"
}
EOF
  terraform apply
done
----

. Wait for bootstrap to complete
+
[source,console]
----
openshift-install --dir "${INSTALLER_DIR}" \
  wait-for bootstrap-complete
----

. Remove bootstrap node and provision infra nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  cat > override.tf <<EOF
module "cluster" {
  worker_count    = 0
  infra_state = "${state}"
}
EOF
  terraform apply
done
----

. Approve infra certs
+
[source,console]
----
export KUBECONFIG="${INSTALLER_DIR}/auth/kubeconfig"

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
kubectl get csr -w
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve

kubectl get nodes

kubectl get nodes -l node-role.kubernetes.io/worker
kubectl label node -l node-role.kubernetes.io/worker \
  node-role.kubernetes.io/infra=""
----

. Provision worker nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  cat > override.tf <<EOF
module "cluster" {
  worker_state = "${state}"
}
EOF
  terraform apply
done
----

. Approve worker certs
+
[source,console]
----
# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
kubectl get csr -w
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve

kubectl get nodes

kubectl label --overwrite node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/app=""
kubectl label node -lnode-role.kubernetes.io/infra \
  node-role.kubernetes.io/app-
----

. Create secret with S3 credentials https://docs.openshift.com/container-platform/4.7/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry]
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${AWS_ACCESS_KEY_ID} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${AWS_SECRET_ACCESS_KEY}
----

. Create wildcard cert for router
+
[source,console]
----
kubectl get secret router-certs-default \
  -n openshift-ingress \
  -o json | \
    jq 'del(.metadata.ownerReferences) | .metadata.name = "router-certs-snakeoil"' | \
  kubectl -n openshift-ingress apply -f -
----

. Save the admin credentials in the https://password.vshn.net[password manager].
You can find the password in the file `target/auth/kubeadmin-password` and the kubeconfig in `target/auth/kubeconfig`
+
[source,console]
----
popd
ls -l ${INSTALLER_DIR}/auth/
----

. https://kb.vshn.ch/vshnsyn/how-tos/synthesize.html[Make the cluster Project Syn enabled]

. Delete local config files
+
[source,console]
----
rm -r ${INSTALLER_DIR}/
----
