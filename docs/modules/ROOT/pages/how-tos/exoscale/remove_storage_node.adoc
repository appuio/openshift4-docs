= Remove a storage node

:kubectl_extra_args: --as=cluster-admin
:delabel_app_nodes: yes
:argo_app: rook-ceph

[abstract]
--
Steps to remove a storage node of an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].
--

== Starting situation

* You already have a OpenShift 4 cluster on Exoscale
* You have admin-level access to the cluster
* You want to remove an existing storage node in the cluster

== Prerequisites

The following CLI utilities need to be available locally:

* `docker`
* `curl`
* `kubectl`
* `oc`
* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `commodore`, see https://syn.tools/commodore/running-commodore.html[Running Commodore]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)

== Prepare local environment

. Create local directory to work in
+
[TIP]
====
We strongly recommend creating an empty directory, unless you already have a work directory for the cluster you're about to work on.
This guide will run Commodore in the directory created in this step.
====
+
[source,bash]
----
export WORK_DIR=/path/to/work/dir
mkdir -p "${WORK_DIR}"
pushd "${WORK_DIR}"
----

. Configure API access
+
include::partial$exoscale/environment-vars.adoc[]
+
include::partial$vshn-input.adoc[]

. Get required tokens from Vault
+
include::partial$connect-to-vault.adoc[]
+
include::partial$get-hieradata-token-from-vault.adoc[]

. Compile the catalog for the cluster.
Having the catalog available locally enables us to run Terraform for the cluster to make any required changes.
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----

== Set alert silence

:duration: +60 minutes
include::partial$create-alertmanager-silence.adoc[]

== Update Cluster Config

. Update cluster config. 
+
[source,bash]
----
pushd "inventory/classes/${TENANT_ID}/"

yq eval -i ".parameters.openshift4_terraform.terraform_variables.storage_count -= 1" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.rook_ceph.ceph_cluster.node_count -= 1" \
  ${CLUSTER_ID}.yml

----
+
[NOTE]
====
Ceph can't scale below 3 storage nodes, which is the default number of nodes.
Please check that this update doesn't reduce the number of storage nodes below 3, before continuing.
====

. Review and commit
+
[source,bash]
----

# Have a look at the file ${CLUSTER_ID}.yml.

git commit -a -m "Remove storage node to cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push cluster catalog 
+
[source,bash]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

== Prepare Terraform environment

include::partial$exoscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]

== Remove Node
Make a note of the node you want to remove

[source,bash]
----
export NODE_TO_REMOVE=storage-XXXX
----

=== Remove old OSD
. Make sure ArgoCD ran and reduced the target number of OSDs
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster \
  get cephcluster cluster -o jsonpath='{.spec.storage.storageClassDeviceSets[0].count}'
----

:osd-node: ${NODE_TO_REMOVE}
include::partial$storage-ceph-remove-osd.adoc[]

. Make a note of the PVC of the old OSD
+
NOTE: We also extract the name of the PV here, but we'll only delete the PV after removing the node from the cluster.
+
[source,bash]
----
pvc_name=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  "rook-ceph-osd-${OSD_ID}" -ojsonpath='{.metadata.labels.ceph\.rook\.io/pvc}')
pv_name=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pvc \
  "${pvc_name}" -o jsonpath='{.spec.volumeName}')
----

. Check if the OSD deployment needs to be deleted, and delete it if necessary
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  -l failure-domain="${NODE_TO_REMOVE}"
# Run this command if the previous command lists a deployment
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete deploy \
  -l failure-domain="${NODE_TO_REMOVE}"
----

. Clean up PVC and prepare job of the old OSD if necessary
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete job \
  -l ceph.rook.io/pvc="${pvc_name}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete pvc "${pvc_name}"
----

. Clean up PVC encryption secret
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete secret -l pvc_name="${pvc_name}"
----

=== Remove the old MON

:mon_node: ${NODE_TO_REMOVE}
include::partial$storage-ceph-remove-mon.adoc[]

=== Remove VM


. Find Terraform resource index of the node to replace
+
[source,bash]
----
# Grab JSON copy of current Terraform state
terraform state pull > .tfstate.json
node_index=$(jq --arg storage_node "${NODE_TO_REMOVE}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="random_id") |
   .instances[] |
   select(.attributes.hex==$storage_node) |
   .index_key' \
  .tfstate.json)
----

. Verify that resource index is correct
+
[source,bash]
----
jq --arg index "${node_index}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="exoscale_compute") |
   .instances[$index|tonumber] |
   .attributes.hostname' \
   .tfstate.json
----

. Remove node ID and node resource for node that we want to remove from the Terraform state
+
[source,bash]
----
terraform state rm "module.cluster.module.storage.random_id.node_id[$node_index]"
terraform state rm "module.cluster.module.storage.exoscale_compute.nodes[$node_index]"
----

:delete-node: ${NODE_TO_REMOVE}
include::partial$exoscale/delete-node.adoc[]

. Clean up localstorage PV of decommissioned node
+
[source,bash]
----
kubectl --as=cluster-admin delete pv "${pv_name}"
----

== Finish up

include::partial$remove-alertmanager-silence.adoc[]


== Upstream documentation

* Rook documentation
** https://rook.io/docs/rook/v1.6/ceph-osd-mgmt.html#remove-an-osd[Remove an OSD]
** https://rook.io/docs/rook/v1.6/ceph-mon-health.html#failing-over-a-monitor[MON failover]
