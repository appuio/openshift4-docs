=== Create a new node

. Find Terraform resource index of the node to replace
+
ifeval::["{cloud_provider}" == "exoscale"]
[source,bash]
----
TF_MODULE="module.cluster.module.storage"
----
endif::[]
ifeval::["{cloud_provider}" == "cloudscale"]
[source,bash]
----
TF_MODULE='module.cluster.module.additional_worker["storage"]' <1>
----
<1> Select the correct worker group.
This guide assumes that your storage nodes are part of an additional worker group called "storage".
endif::[]
+
[source,bash]
----
# Grab JSON copy of current Terraform state
terraform state pull > .tfstate.json
node_index=$(jq --arg tfmodule "${TF_MODULE}" --arg storage_node "${NODE_TO_REPLACE}" -r \
  '.resources[] |
   select(.module==$tfmodule and .type=="random_id") |
   .instances[] |
   select(.attributes.hex==$storage_node) |
   .index_key' \
  .tfstate.json)
----

. Verify that resource index is correct
+
ifeval::["{cloud_provider}" == "cloudscale"]
[source,bash]
----
jq --arg tfmodule "${TF_MODULE}" --arg index "${node_index}" -r \
  '.resources[] |
   select(.module==$tfmodule and .type=="cloudscale_server") |
   .instances[$index|tonumber] |
   .attributes.name' \
   .tfstate.json
----
endif::[]
ifeval::["{cloud_provider}" == "exoscale"]
[source,bash]
----
jq --arg tfmodule "${TF_MODULE}" --arg index "${node_index}" -r \
  '.resources[] |
   select(.module==$tfmodule and .type=="exoscale_compute_instance") |
   .instances[$index|tonumber] |
   .attributes.name' \
   .tfstate.json
----
endif::[]

. Remove node ID and node resource for node that we want to replace from the Terraform state
+
ifeval::["{cloud_provider}" == "exoscale"]
[source,bash]
----
terraform state rm "module.cluster.module.storage.random_id.node_id[$node_index]"
terraform state rm "module.cluster.module.storage.exoscale_compute_instance.nodes[$node_index]"
----
endif::[]
ifeval::["{cloud_provider}" == "cloudscale"]
[source,bash]
----
terraform state rm "${TF_MODULE}.random_id.node[$node_index]"
terraform state rm "${TF_MODULE}.cloudscale_server.node[$node_index]"
----
endif::[]

. Run Terraform to spin up a replacement node
+
[source,bash]
----
terraform apply
----

. Approve node cert for new storage node
+
include::partial$install/approve-node-csrs.adoc[]

. Label and taint the new storage node
+
include::partial$label-taint-storage-nodes.adoc[]

ifeval::["{cloud_provider}" == "exoscale"]
. Wait for the localstorage PV on the new node to be created
+
[source,bash]
----
kubectl --as=cluster-admin get pv \
  -l storage.openshift.com/local-volume-owner-name=storagevolumes -w
----

. Disable auto sync for component `rook-ceph`.
This allows us to temporarily make manual changes to the Rook Ceph cluster.
+
include::partial$disable-argocd-autosync.adoc[]

. Make a note of the original count of OSDs in the Ceph cluster
+
[source,bash]
----
orig_osd_count=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster \
  get cephcluster cluster -o jsonpath='{.spec.storage.storageClassDeviceSets[0].count}')
----

. Change Ceph cluster to have one more OSD
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p "[{
    \"op\": \"replace\",
    \"path\": \"/spec/storage/storageClassDeviceSets/0/count\",
    \"value\": $(expr ${orig_osd_count} + 1)
  }]"
----

. Wait until the new OSD is launched
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -w
----
// "{cloud_provider}" == "exoscale"
endif::[]
