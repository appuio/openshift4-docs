
. Deploy bootstrap node
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  master_count             = 0
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" != "stackit"]
. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----
endif::[]
ifeval::["{provider}" == "stackit"]
. Set up DNS NS records on parent zone using the data from the Terraform output variable ns_records from the previous step
+
[WARNING]
--
`component-openshift4-terraform` doesn't currently support the STACKIT cluster Terraform module, so at this time, the Terraform output for NS records isn't provided.
--
endif::[]
ifeval::["{provider}" == "cloudscale"]
. Store the subnet UUID in the cluster configuration
+
[source,bash]
----
export SUBNET_UUID="`terraform output -raw subnet_uuid`"

pushd ../../../inventory/classes/${TENANT_ID}

yq eval -i ".parameters.openshift.cloudscale.subnet_uuid = $SUBNET_UUID" \
  ${CLUSTER_ID}.yml

git commit -am "Add Cloudscale Subnet UUID to cluster configuration for ${CLUSTER_ID}"
git push
popd
popd
----

. Compile and push the cluster catalog
+
include::partial$install/commodore-dynfacts.adoc[]

. Return to Terraform directory
+
[source,bash]
----
pushd catalog/manifests/openshift4-terraform/
----
endif::[]

. Wait for bootstrap API to come up
+
[source,bash]
----
API_URL=$(yq e '.clusters[0].cluster.server' "${INSTALLER_DIR}/auth/kubeconfig")
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Patch Cilium config to allow control plane bootstrap to succeed
+
[NOTE]
====
We need to temporarily adjust the Cilium config to not use full kube-proxy replacement, since we currently don't have a way to disable the initial OpenShift-managed kube-proxy deployment.
Additionally, because the {provider} Cloud Controller Manager accesses the K8s API via service IP, we need to configure Cilium to provide partial kube-proxy replacement so that the CCM can start and untaint the control plane nodes so that other pods can be scheduled.
====
+
[source,bash]
----
export KUBECONFIG="${INSTALLER_DIR}/auth/kubeconfig"

while ! kubectl get ciliumconfig -A &>/dev/null; do
  echo -n "."
  sleep 2
done && echo -e "\nCiliumConfig CR is present"

kubectl patch -n cilium ciliumconfig cilium-enterprise --type=merge \
 -p '{
  "spec": {
    "cilium": {
      "kubeProxyReplacement": "false",
      "nodePort": {
        "enabled": true
      },
      "socketLB": {
        "enabled": true
      },
      "sessionAffinity": true,
      "externalIPs": {
        "enabled": true
      },
      "hostPort": {
        "enabled": true
      }
    }
  }
 }'
----

. Deploy control plane nodes
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" == "cloudscale"]
. Add the DNS records for etcd shown in output variable `dns_entries` from the previous step to the cluster's parent zone

. Apply the manifests for the cloudscale machine-api provider
+
[source,bash,subs="attributes+"]
----
export VAULT_TOKEN=$(vault token lookup - format=json | jq -r .data.id) 
kapitan refs --reveal --refs-path ../../refs -f ../machine-api-provider-cloudscale/00_secrets.yaml | kubectl apply -f -

kubectl apply  -f ../machine-api-provider-cloudscale/10_clusterRoleBinding.yaml

kubectl apply -f ../machine-api-provider-cloudscale/10_serviceAccount.yaml

kubectl apply -f ../machine-api-provider-cloudscale/11_deployment.yaml
----

. Apply the machinesets from the catalog
+
[source,bash,subs="attributes+"]
----
kubectl apply -f ../openshift4-nodes/machineset-*.yaml
----
endif::[]

. Wait for bootstrap to complete
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  wait-for bootstrap-complete --log-level debug
----
+
[NOTE]
--
If you're using a CNI other than Cilium you may need to remove the following taint from the nodes to allow the network to come up:
[source,bash]
----
kubectl taint no --all node.cloudprovider.kubernetes.io/uninitialized:NoSchedule-
----
Once the bootstrap is complete, taint the master nodes again to ensure that they're properly initialized by the cloud-controller-manager.
[source,bash]
----
kubectl taint no -l node-role.kubernetes.io/master node.cloudprovider.kubernetes.io/uninitialized=:NoSchedule
----
--

ifeval::["{provider}" != "cloudscale"]
. Remove bootstrap node and provision remaining nodes
endif::[]
ifeval::["{provider}" == "cloudscale"]
. Remove bootstrap node
endif::[]
+
[source,bash,subs="attributes+"]
----
rm override.tf
terraform apply

popd
----

ifeval::["{provider}" != "stackit"]
. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----
endif::[]

ifeval::["{provider}" == "cloudscale"]
. Scale up the infra and worker machinesets
+
[source,bash]
----
INFRA_NODES=4 # adjust to desired number of infra nodes
WORKER_NODES=3 # adjust to desired number of worker nodes
----
+
[source,bash]
----
kubectl scale machineset -nopenshift-machine-api infra --replicas="${INFRA_NODES}"
kubectl scale machineset -nopenshift-machine-api worker --replicas="${WORKER_NODES}"
----
endif::[]

. Disable OpenShift kube-proxy deployment and revert Cilium patch
+
[source,bash]
----
kubectl patch network.operator cluster --type=merge \
  -p '{"spec":{"deployKubeProxy":false}}'
kubectl -n cilium replace -f catalog/manifests/cilium/olm/cluster-network-07-cilium-ciliumconfig.yaml
while ! kubectl -n cilium get cm cilium-config -oyaml | grep 'kube-proxy-replacement: "true"' &>/dev/null; do
  echo -n "."
  sleep 2
done && echo -e "\nCilium config updated"
kubectl -n cilium rollout restart ds/cilium
----

ifeval::["{provider}" == "cloudscale"]
. Add Infra Node IPs to LB Hieradata
+
[source,bash]
----
git clone git@git.vshn.net:appuio/appuio_hieradata.git

pushd appuio_hieradata/lbaas

# Use this with OCP
kubectl get node -l "node-role.kubernetes.io/infra" -oyaml | yq '.items[].status.addresses | filter(.type == "InternalIP") | map(.address)' > ips.yml

# Use this with OKE, since OKE does not have infra nodes and workers have the ingress deployed by default
kubectl get node -l "node-role.kubernetes.io/worker" -oyaml | yq '.items[].status.addresses | filter(.type == "InternalIP") | map(.address)' > ips.yml


yq -i '."profile_openshift4_gateway::backends".router = load("ips.yml")' "${CLUSTER_ID}.yaml"

rm ips.yml

git commit -am "Add infra nodes as backends for ${CLUSTER_ID}."
git push
popd
----
endif::[]
ifeval::["{provider}" != "cloudscale"]
. Approve node certs
+
include::partial$install/approve-node-csrs.adoc[]

. Label infra nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra-")).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/infra=
----

ifeval::["{provider}" == "exoscale"]
. Label and taint storage nodes
+
include::partial$label-taint-storage-nodes.adoc[]
endif::[]

. Label worker nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra|master|storage-")|not).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/app=
----
+
[NOTE]
At this point you may want to add extra labels to the additional worker groups, if there are any.
endif::[]

. Enable proxy protocol on ingress controller
+
[source,bash]
----
kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/endpointPublishingStrategy",
    "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
  }]'
----
+
[TIP]
====
This step isn't necessary if you've disabled the proxy protocol on the load-balancers manually during setup.

By default, PROXY protocol is enabled through the VSHN Commodore global defaults.
====

. Wait for installation to complete
+
[source,bash]
----
openshift-install --dir ${INSTALLER_DIR} \
  wait-for install-complete --log-level debug
----
