= Add a storage node

:kubectl_extra_args: --as=cluster-admin
:delabel_app_nodes: yes
:argo_app: rook-ceph

[abstract]
--
Steps to add a storage node to an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].
--

== Starting situation

* You already have a OpenShift 4 cluster on Exoscale
* You have admin-level access to the cluster
* You want to add a new storage node to the cluster

== Prerequisites

The following CLI utilities need to be available locally:

* `docker`
* `curl`
* `kubectl`
* `oc`
* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `commodore`, see https://syn.tools/commodore/running-commodore.html[Running Commodore]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)

== Prepare local environment

. Create local directory to work in
+
[TIP]
====
We strongly recommend creating an empty directory, unless you already have a work directory for the cluster you're about to work on.
This guide will run Commodore in the directory created in this step.
====
+
[source,bash]
----
export WORK_DIR=/path/to/work/dir
mkdir -p "${WORK_DIR}"
pushd "${WORK_DIR}"
----

. Configure API access
+
include::partial$exoscale/environment-vars.adoc[]
+
include::partial$vshn-input.adoc[]

. Get required tokens from Vault
+
include::partial$connect-to-vault.adoc[]
+
include::partial$get-hieradata-token-from-vault.adoc[]
+
[source,bash]
----
export TF_VAR_lb_exoscale_api_user=$(vault kv get \
  -format=json "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/floaty" | jq '.data.data')
export TF_VAR_lb_exoscale_api_key=$(echo "${TF_VAR_lb_exoscale_api_user}" | jq -r '.iam_key')
export TF_VAR_lb_exoscale_api_secret=$(echo "${TF_VAR_lb_exoscale_api_user}" | jq -r '.iam_secret')
----

. Compile the catalog for the cluster.
Having the catalog available locally enables us to run Terraform for the cluster to make any required changes.
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----


== Set alert silence

:duration: +60 minutes
include::partial$create-alertmanager-silence.adoc[]

== Update Cluster Config

. Update cluster config. 
+
[source,bash]
----
pushd "inventory/classes/${TENANT_ID}/"

yq eval -i ".parameters.openshift4_terraform.terraform_variables.storage_count = 
  (.parameters.openshift4_terraform.terraform_variables.storage_count // 3) + 1" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.rook_ceph.ceph_cluster.node_count = 
  (.parameters.rook_ceph.ceph_cluster.node_count // 3) + 1" \
  ${CLUSTER_ID}.yml

----

. Review and commit
+
[source,bash]
----

# Have a look at the file ${CLUSTER_ID}.yml.

git commit -a -m "Add storage node to cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push cluster catalog 
+
[source,bash]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

== Prepare Terraform environment

include::partial$exoscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]



== Add node

. Run Terraform to spin up a new node
+
[source,bash]
----
terraform apply
----

. Approve node cert for new storage node
+
include::partial$install/approve-node-csrs.adoc[]

. Label and taint the new storage node
+
[source,bash]
----
# Find the new storage node
kubectl --as=cluster-admin get nodes

# Label the new storage node
kubectl --as=cluster-admin label node <storage-XXX>\
  node-role.kubernetes.io/storage=""

kubectl --as=cluster-admin taint node -lnode-role.kubernetes.io/storage \
  storagenode=True:NoSchedule
----

. Wait until the new OSD is launched.
This requires ArgoCD to have run and the Rook-Ceph operator to notice the change.
This might take a few minutes.
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -w
----

. Wait for the data to be redistributed ("backfilled") to the new OSD.
+
include::partial$storage-ceph-backfilling.adoc[]

== Finish up

include::partial$remove-alertmanager-silence.adoc[]
