
. Deploy bootstrap node
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  master_count             = 0
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----

. Wait for bootstrap API to come up
+
[source,bash]
----
API_URL=$(yq e '.clusters[0].cluster.server' "${INSTALLER_DIR}/auth/kubeconfig")
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" == "cloudscale"]
. Add the DNS records for etcd shown in output variable `dns_entries` from the previous step to the cluster's parent zone
endif::[]

. Wait for bootstrap to complete
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  wait-for bootstrap-complete --log-level debug
----

. Remove bootstrap node and provision infra nodes
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Approve infra certs
+
[source,bash]
----
export KUBECONFIG="${INSTALLER_DIR}/auth/kubeconfig"
----
+
include::partial$install/approve-node-csrs.adoc[]

. Label infra nodes
+
[source,bash]
----
kubectl get nodes -lnode-role.kubernetes.io/worker
kubectl label node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/infra=""
----

. Enable proxy protocol on ingress controller
+
[source,bash]
----
kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/endpointPublishingStrategy",
    "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
  }]'
----
+
[TIP]
====
This step isn't necessary if you've disabled the proxy protocol on the load-balancers manually during setup.

By default, PROXY protocol is enabled through the VSHN Commodore global defaults.
====

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----

. Wait for installation to complete
+
[source,bash]
----
openshift-install --dir ${INSTALLER_DIR} \
  wait-for install-complete --log-level debug
----

ifeval::["{provider}" == "exoscale"]
. Provision storage nodes
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Approve storage certs
+
include::partial$install/approve-node-csrs.adoc[]

. Label and taint storage nodes
+
include::partial$label-taint-storage-nodes.adoc[]
endif::[]

. Provision worker nodes
+
[source,bash]
----
rm override.tf
terraform apply

popd
----

. Approve worker certs
+
include::partial$install/approve-node-csrs.adoc[]

. Label worker nodes
+
[source,bash,subs="attributes"]
----
kubectl label --overwrite node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/app=""
kubectl label node -lnode-role.kubernetes.io/infra \
  node-role.kubernetes.io/app-
ifeval::["{provider}" == "exoscale"]
kubectl label node -lnode-role.kubernetes.io/storage \
  node-role.kubernetes.io/app-
endif::[]

# This should show the worker nodes only
kubectl get nodes -l node-role.kubernetes.io/app
----
+
[NOTE]
At this point you may want to add extra labels to the additional worker groups, if there are any.
