= Uninstallation on cloudscale

:provider: cloudscale

[abstract]
--
Steps to remove an OpenShift 4 cluster from https://cloudscale[cloudscale.ch].
--

[NOTE]
--
- The commands are idempotent and can be retried if any of the steps fail.
- In the future, this procedure will be mostly automated
--

== Prerequisites

include::partial$cloudscale/prerequisites.adoc[]
* `emergency-credentials-receive` https://github.com/vshn/emergency-credentials-receive?tab=readme-ov-file#install-from-binary[Install instructions]

== Cluster Decommission

. Setup environment variables for GitLab and Vault credentials
+
[source,bash]
----
export GITLAB_TOKEN=<gitlab-api-token> # From https://git.vshn.net/-/user_settings/personal_access_tokens
export GITLAB_USER=<gitlab-user-name>
----
+
include::partial$connect-to-vault.adoc[]

include::partial$commodore-init.adoc[]

. Setup environment variables for cloudscale credentials
+
[source,bash]
----
export CLOUDSCALE_API_TOKEN=$(vault kv get -format=json clusters/kv/$TENANT_ID/$CLUSTER_ID/cloudscale | jq -r .data.data.token)
export REGION=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .facts.region)
export BACKUP_REGION=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" https://api.cloudscale.ch/v1/regions | jq -r '.[].slug' | grep -v $REGION)
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq -r '.data.data.token')
----

include::partial$decommission-disable-syn.adoc[]

. Delete all LB services
+
[source,bash]
----
kubectl delete svc --field-selector spec.type=LoadBalancer -A
----

. Delete all PVs
+
[source,bash]
----
kubectl cordon -l node-role.kubernetes.io/worker
kubectl get po -A -oyaml | yq '.items = [.items[] |
    select(.spec.nodeName | test("master-") | not) |
    select(.metadata.namespace != "syn-csi-cloudscale")]' |\
    kubectl delete --wait=false -f-
kubectl delete pvc -A --all --wait=false
kubectl wait --for=delete pv --all --timeout=120s
----
+
NOTE: By cordoning all non-master nodes and deleting all their pods (except the csi driver pods) we ensure that no new PVs are created, while the existing ones can be cleaned up.
Deleting all pods has the additional benefit that we don't have to deal with PDBs when deleting the machinesets in the next step.
+
[NOTE]
====
Delete remaining PVs explicitly if not all PVs were deleted in the previous step.

[source,bash]
----
kubectl delete pv --all
----
====

. Delete all machinesets
+
[source,bash]
----
kubectl -n openshift-machine-api delete machinesets --all
kubectl -n openshift-machine-api wait --for=delete \
    machinesets,machines --all --timeout=120s
kubectl get nodes
----

. Configure Terraform secrets
+
[source,bash]
----
cat <<EOF > ./terraform.env
CLOUDSCALE_API_TOKEN
HIERADATA_REPO_TOKEN
EOF
----

include::partial$setup_terraform.adoc[]

include::partial$prepare-for-lb-decommission.adoc[]

. Delete resources from cloudscale using Terraform
+
[source,bash]
----
# The first time it will fail
terraform destroy
# Destroy a second time to delete private networks
terraform destroy
----
+
[source,bash]
----
popd
----

. After all resources are deleted we need to remove the buckets
+
[source,bash]
----
# Use already exiting bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")

# configure minio client to use the bucket
mc config host add \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

# delete bootstrap-ignition bucket (should already be deleted after setup)
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition" --force

# delete image-registry object
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-image-registry" --force
----

. Delete the cluster-backup bucket in the cloudscale project
+
[NOTE]
====
Verify that the cluster backups aren't needed anymore before cleaning up the backup bucket.
Consider extracting the most recent cluster objects and etcd backups before deleting the bucket.
See the xref:how-tos/recover-from-backup.adoc[Recover objects from backup] how-to for instructions.
At this point in the decommissioning process, you'll have to extract the Restic configuration from Vault instead of the cluster itself.
====
+
[source,bash]
----
# configure minio client to use the bucket
mc config host add \
  "${CLUSTER_ID}_backup" "https://objects.${BACKUP_REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

mc rb "${CLUSTER_ID}_backup/${CLUSTER_ID}-cluster-backup" --force

# delete cloudscale object user
curl -i -H "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" -X DELETE $(echo $response | jq -r '.href')
----

. Delete all remaining volumes which were associated with the cluster in the cloudscale project.
+
TIP: This step is required because the csi-cloudscale driver doesn't have time to properly cleanup PVs when the cluster is decommissioned with `terraform destroy`.

. Delete the cluster's API tokens in the cloudscale UI

include::partial$vshn-decommission.adoc[]
