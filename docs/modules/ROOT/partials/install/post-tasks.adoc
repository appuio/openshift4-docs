=== Post tasks

==== VSHN

. Verify that an `UpgradeConfig` is present
+
[source,bash]
----
kubectl -n appuio-openshift-upgrade-controller get upgradeconfig
----
+
TIP: Double-check the cluster's `maintenance_window` fact, if this command doesn't return any objects.

. Schedule a first maintenance 1 minute in the future
+
[source,bash]
----
uc=$(yq .parameters.facts.maintenance_window inventory/classes/params/cluster.yml)
kubectl -n appuio-openshift-upgrade-controller get upgradeconfig $uc -oyaml | \
  yq '
    .metadata.name = "first", <1>
    .metadata.labels = {}, <2>
    .spec.jobTemplate.metadata.labels.upgradeconfig/name = "first", <1>
    .spec.schedule.cron = ((now+"1m")|format_datetime("4 15")) + " * * *", <3>
    .spec.pinVersionWindow = "0m" <4>
  ' | \
  kubectl create -f - --as=system:admin
----
<1> The name doesn't matter, but the `upgradeconfig/name` label in the job template must match `metadata.name` of the copied `UpgradeConfig`.
<2> We clear the resource labels so ArgoCD doesn't delete the copied resource.
<3> This expression converts `now+1m` to a valid cronspec for daily runs at that time of day.
<4> We set the `pinVersionWindow` to 0 minutes to ensure that the first job actually gets scheduled one minute in the future.
+
[WARNING]
====
Don't forget to delete the copied `UpgradeConfig` resource after the initial maintenance completes.

[source,bash]
----
kubectl --as=system:admin -n appuio-openshift-upgrade-controller \
  delete upgradeconfig first
----
====

. Add cluster in https://git.vshn.net/vshn/openshift4-clusters[openshift4-clusters (internal)].

.. Setup basic config for the cluster
+
[source,bash]
----
git clone git@git.vshn.net:vshn/openshift4-clusters.git
pushd openshift4-clusters
mkdir "${CLUSTER_ID}"
pushd "${CLUSTER_ID}"
ln -s ../base_envrc .envrc
cat >.connection_facts <<EOF
API=${API_URL}
EOF
popd
----
+
.. (Optional) Configure SOCKS5 SSH proxy jumphost
+
[source,bash,sub="attributes+"]
----
SOCKS5_PORT=120xy <1>
cat >> "${CLUSTER_ID}/.connection_facts <<EOF
JUMPHOST=${JUMPHOST_FQDN}
SOCKS5_PORT=${SOCKS5_PORT}
ifeval::["{provider}"=="vsphere"]
VCENTER=https://${VCENTER_HOSTNAME}
endif::[]
EOF
python foxyproxy_generate.py <2>
----
<1> Make sure to pick a new SOCKS5 proxy port if the cluster you're setting up requires a proxy jumphost that's not yet used by any other cluster.
Correspondingly, copy the port from an existing cluster, if there's existing clusters that use the same proxy jumphost.
<2> Regenerate the FoxyProxy config after setting up the new cluster's SOCKS5 proxy jumphost.

.. (Optional) Adjust the `.connection_facts` for special setups, if necessary.
For example, configure the VPN connection to use to access the cluster.

.. Test the config
+
NOTE: This assumes that you've already configured `direnv` for the `openshift4-clusters` repo.
+
NOTE: If you've regenerated the FoxyProxy config, import it in your browser before testing the config.
+
[source,bash]
----
pushd "${CLUSTER_ID}"
direnv reload
oc whoami
kubectl get nodes
popd
----

.. Commit and push the changes
+
[source,bash]
----
git commit -am "Add cluster ${CLUSTER_ID}"
git push
popd
----

==== Generic

. Do a https://docs.openshift.com/container-platform/latest/updating/updating_a_cluster/updating-cluster-cli.html#update-upgrading-cli_updating-cluster-cli[first maintenance]
