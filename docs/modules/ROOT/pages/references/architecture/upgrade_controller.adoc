= Upgrade Controller

== Problem Statement

Maintenance of OpenShift 4 clusters is a manual process.
We promise maintenance windows outside office hours, and some of them are Switzerland only.

Staying up late to upgrade those clusters is not a sustainable solution.
It binds team members to the task, and it is not a good use of their time.

The need for automation is obvious.
We decided to write xref:oc4:ROOT:explanations/decisions/maintenance-trigger.adoc[our own upgrade controller].

== High Level Goals

* *A normal, successful upgrade is done without any manual intervention during a defined maintenance window*
* Maintenance window and upgrade rhythm are configurable on a per-cluster basis
** Suspending upgrades is possible
* Maintenance engineers are notified when an upgrade fails
** Maintenance is skipped when cluster is unhealthy

== Non-Goals

* More general centralized management of OpenShift 4 clusters

== Implementation

The controller is a standard `controller-runtime` controller.
It is deployed on each OpenShift 4 cluster.
It is managed through a Custom Resource Definition (CRD) called `UpgradeConfig`.

=== The controller is extendable through webhooks

The controller should be able to send notifications to a webhook.
Every step of the upgrade process should be notified.

=== The controller manages the content of the `ClusterVersion/version` object

The `ClusterVersion/version` object is the source of truth for the cluster's current version and available updates.
It is currently managed by ArgoCD which could conflict with the controller.
The controller should replace ArgoCD and manage the object from its own CRD.

The https://github.com/openshift/api/blob/1957a8d7445bf2332f027f93a24d7573f77a0dc0/config/v1/types_cluster_version.go#L35[configv1.ClusterVersionSpec] is included in the `UpgradeConfig` CRD and syncs the `ClusterVersion/version` object.

The `.spec.desiredUpdate` field is set to start the upgrade.

=== The controller pins the upgrade at a time before the maintenance

The controller creates an `UpgradeJob` object at a time configured in the `UpgradeConfig` object.
The `UpgradeJob` contains a snapshot of the most recent version in the `.status.availableUpdates` field and a timestamp when the upgrade should start.

The `UpgradeJob` rechecks the available updates at the time of the upgrade.
If the version is no longer available, the upgrade is skipped and a notification is send to the webhook.

=== Interval/ time window definition

* The controller must support customizable upgrade start time
* The controller must be able to support various upgrade rhythms (weekly, every two weeks, whenever there's an update)

The upgrade start time is defined in the `UpgradeConfig` object.
It is in the form of a cron expression with an additional field for the https://www.iso.org/obp/ui#iso:std:iso:8601:-1:ed-1:v1:en:term:3.1.1.23[ISO 8601 week number] (https://pkg.go.dev/time#Time.ISOWeek[time#Time.ISOWeek]).
The additional field is used to define the weekly upgrade rhythm.
The syntax is cron-like, e.g. `7` means on the 7th week of the year.
The initial implementation will support only `@odd` and `@even` which means every odd/even week of the year.

We sell maintenance windows at the local time of the customer.
The time zone of the schedule should be configurable.

It must be possible to suspend scheduling of upgrades.

[source,yaml]
----
schedule:
  cron: "0 22 * * 2" # 22:00 on Tuesdays
  isoWeek: "@odd" # every odd week of the year
  location: "Europe/Zurich" # time at this location
  suspend: false # whether to suspend timing new upgrades
----

=== The controller verifies cluster health before and after the upgrade

The controller should not try to upgrade a cluster that is not healthy.

A `UpgradeJob` checks the cluster health before the upgrade and skips the upgrade if the cluster is unhealthy.
if an update is skipped, the controller should send a notification to the webhook.

The controller should also check the cluster health after the upgrade.
If the cluster is unhealthy, the controller should send a notification to the webhook.

[source,yaml]
----
preUpgradeHealthChecks:
  checkCriticalAlerts: true
  checkDegradedOperators: true
  excludeAlerts:
  - alertname: "KubePodCrashLooping"
  excludeNamespaces:
  - openshift-console
  excludeOperators:
  - openshift-monitoring
----

==== Query alerts

The controller should query the cluster's Prometheus instance for alerts.
If there are any alerts with `severity=critical`, the cluster is unhealthy.

It should be possible to exclude alerts and alerts in namespaces.

==== Check cluster operator health

KJDHFKJLDSHFLKJSDHFKLSDHFKLJSHDFLKSHDFKLJHSDLKFHSDKHFLSKDFHJSDFHl


** If the cluster is unhealthy before the maintenance, the upgrade should be skipped
* The controller must expose Prometheus metrics indicating current state of upgrade
** It must be possible to create alerts for failures in the upgrade process, e.g. upgrade triggering failed, upgrade itself failed, upgrade got stuck, ...
* The controller should have extension points before/after each upgrade phase
** This will allow us to easily accommodate customer requests, e.g. POSTing to some HTTP API after the upgrade is done

== Resources

- https://access.redhat.com/labs/ocpupgradegraph/update_channel[RedHat OCP Upgrade Graph]
- https://github.com/openshift/managed-upgrade-operator/blob/fc03c10ca7bb95f15a9b7c7d75bd129fb0d4fef4/pkg/upgraders/healthcheckstep.go#L51[RedHat Managed Upgrade Operator Health Check]
