
. Deploy bootstrap node
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  master_count             = 0
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----

. Wait for bootstrap API to come up
+
[source,bash]
----
API_URL=$(yq e '.clusters[0].cluster.server' "${INSTALLER_DIR}/auth/kubeconfig")
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,bash,subs="attributes+"]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 1
  infra_count              = 0
ifeval::["{provider}" == "exoscale"]
  storage_count            = 0
endif::[]
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

ifeval::["{provider}" == "cloudscale"]
. Add the DNS records for etcd shown in output variable `dns_entries` from the previous step to the cluster's parent zone
endif::[]

. Wait for bootstrap to complete
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  wait-for bootstrap-complete --log-level debug
----
+
[NOTE]
--
If you're using a CNI other than Cilium you may need to remove the following taint from the nodes to allow the network to come up:
[source,bash]
----
kubectl taint no --all node.cloudprovider.kubernetes.io/uninitialized:NoSchedule-
----
Once the bootstrap is complete, taint the master nodes again to ensure that they're properly initialized by the cloud-controller-manager.
[source,bash]
----
kubectl taint no -l node-role.kubernetes.io/master node.cloudprovider.kubernetes.io/uninitialized=:NoSchedule
----
--

. Remove bootstrap node and provision remaining nodes
+
[source,bash,subs="attributes+"]
----
rm override.tf
terraform apply

popd
----

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and run Puppet on the LBs after the deploy job has completed
+
[source,bash]
----
for fqdn in "${LB_FQDNS[@]}"; do
  ssh "${fqdn}" sudo puppetctl run
done
----

. Approve node certs
+
[source,bash]
----
export KUBECONFIG="${INSTALLER_DIR}/auth/kubeconfig"
----
+
include::partial$install/approve-node-csrs.adoc[]

. Label infra nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra-")).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/infra=
----

ifeval::["{provider}" == "exoscale"]
. Label and taint storage nodes
+
include::partial$label-taint-storage-nodes.adoc[]
endif::[]

. Label worker nodes
+
[source,bash]
----
kubectl get node -ojson | \
  jq -r '.items[] | select(.metadata.name | test("infra|master|storage-")|not).metadata.name' | \
  xargs -I {} kubectl label node {} node-role.kubernetes.io/app=
----
+
[NOTE]
At this point you may want to add extra labels to the additional worker groups, if there are any.

. Enable proxy protocol on ingress controller
+
[source,bash]
----
kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/endpointPublishingStrategy",
    "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
  }]'
----
+
[TIP]
====
This step isn't necessary if you've disabled the proxy protocol on the load-balancers manually during setup.

By default, PROXY protocol is enabled through the VSHN Commodore global defaults.
====

. Wait for installation to complete
+
[source,bash]
----
openshift-install --dir ${INSTALLER_DIR} \
  wait-for install-complete --log-level debug
----
