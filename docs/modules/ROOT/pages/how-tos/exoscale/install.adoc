= Installation on Exoscale
:ocp-minor-version: 4.11
:k8s-minor-version: 1.24
:ocp-patch-version: {ocp-minor-version}.9
:provider: exoscale
:kubectl_extra_args:

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its Git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
+
IMPORTANT: Don't use your personal account to login to the cluster manager for installation.
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Exoscale

== Prerequisites

include::partial$install/prerequisites.adoc[]
* `md5sum`
* `virt-edit`
* `cpio`
//* Clone of the https://github.com/appuio/terraform-openshift4-exoscale[terraform-openshift4-exoscale] repository
* `exo` >= v1.48.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* An unrestricted Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key]
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

include::partial$install/register.adoc[]

=== Configure input

include::partial$exoscale/environment-vars.adoc[]

include::partial$install/vshn-input.adoc[]

[#_create_iam_keys]
=== Create restricted Exoscale IAM keys for the LBs and object storage

. Create restricted API key for Exoscale object storage
+
[source,bash]
----
exoscale_s3_credentials=$(exo iam access-key create "${CLUSTER_ID}_object_storage" \
  --tag sos -O json)
export EXOSCALE_S3_ACCESSKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.api_key')
export EXOSCALE_S3_SECRETKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.api_secret')
----

[#_bootstrap_bucket]
=== Set up S3 bucket for cluster bootstrap

. Create S3 bucket
+
[source,bash]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap" --zone "${EXOSCALE_ZONE}"
----

[#_upload_coreos_image]
=== Upload Red Hat CoreOS image

. Fetch and convert the latest Red Hat CoreOS image
+
[source,bash,subs="attributes+"]
----
RHCOS_VERSION="{ocp-patch-version}"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/{ocp-minor-version}/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo compute instance-template register "rhcos-${RHCOS_VERSION}" \
  "https://${EXOSCALE_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2" \
  "$(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }')" \
  --zone "${EXOSCALE_ZONE}" \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export RHCOS_TEMPLATE="rhcos-${RHCOS_VERSION}"
----


[#_set_vault_secrets]
=== Set secrets in Vault

include::partial$connect-to-vault.adoc[]

.Store various secrets in Vault
[source,bash]
----
# Set the Exoscale object storage API key
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale/storage_iam \
  s3_access_key=${EXOSCALE_S3_ACCESSKEY} \
  s3_secret_key=${EXOSCALE_S3_SECRETKEY}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -

# Copy the VSHN acme-dns registration password
vault kv get -format=json "clusters/kv/template/cert-manager" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cert-manager" -
----

include::partial$get-hieradata-token-from-vault.adoc[]

include::partial$install/prepare-commodore.adoc[]

[#_configure_installer]
=== Configure the OpenShift Installer

include::partial$install/configure-installer.adoc[]

[#_run_installer]
=== Run OpenShift Installer

include::partial$install/run-installer.adoc[]

. Upload ignition config
+
[source,bash]
----
exo storage upload "${INSTALLER_DIR}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read
----

=== Terraform Cluster Config

include::partial$install/prepare-terraform.adoc[]

[#_compile_catalog]
=== Commit changes and compile cluster catalog

. Review changes.
Have a look at the file `${CLUSTER_ID}.yml`.
Override default parameters or add more component configurations as required for your cluster.
+
[IMPORTANT]
====
Ensure that you're using component `openshift4-terraform` v5.0.0 or newer.
Otherwise the instructions in this how-to might not apply.
====

. Commit changes
+
[source,bash]
----
git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push cluster catalog
+
include::partial$install/commodore-dynfacts.adoc[]


=== Provision Infrastructure

include::partial$exoscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]

. Provision Domain and security groups
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 0
  lb_count                 = 0
  master_count             = 0
  infra_count              = 0
  storage_count            = 0
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Set Exoscale Elastic IP reverse DNS
+
[source,bash]
----
terraform state pull > state.json

ingress_id=$(jq -r '.resources[] | select(.type == "exoscale_elastic_ip" and .name == "ingress") | .instances[0].attributes.id' state.json)
api_id=$(jq -r '.resources[] | select(.type == "exoscale_elastic_ip" and .name == "api") | .instances[0].attributes.id' state.json)
# Use auto-generated Exoscale API V2 OpenAPI command to set EIP reverse DNS
echo "{\"domain-name\": \"ingress.${CLUSTER_DOMAIN}.\"}" | exo x --zone "${EXOSCALE_ZONE}" update-reverse-dns-elastic-ip "$ingress_id"
echo "{\"domain-name\": \"api.${CLUSTER_DOMAIN}.\"}" | exo x --zone "${EXOSCALE_ZONE}" update-reverse-dns-elastic-ip "$api_id"

rm state.json
----

. Create LB hieradata
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 0
  master_count             = 0
  infra_count              = 0
  storage_count            = 0
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply -target "module.cluster.module.lb.module.hiera"
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records` from the previous step

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and wait until the deploy pipeline after the merge is completed.

. Create LBs
+
[source,bash]
----
terraform apply
----

. Make LB FQDNs available for later steps
+
.Store LB FQDNs in environment
[source,bash]
----
declare -a LB_FQDNS
for id in 1 2; do
  LB_FQDNS[$id]=$(terraform state show "module.cluster.module.lb.exoscale_domain_record.lb[$(expr $id - 1)]" | grep hostname | cut -d'=' -f2 | tr -d ' "\r\n')
done
----
+
.Verify FQDNs
[source,bash]
----
for lb in "${LB_FQDNS[@]}"; do echo $lb; done
----

include::partial$install/bootstrap-lb.adoc[]

include::partial$install/bootstrap-nodes.adoc[]

. Create secret with S3 credentials https://docs.openshift.com/container-platform/{ocp-minor-version}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry]
+
[source,bash]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${EXOSCALE_S3_ACCESSKEY} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${EXOSCALE_S3_SECRETKEY}
----
+
include::partial$install/registry-samples-operator.adoc[]

include::partial$install/finalize_part1.adoc[]

include::partial$install/finalize_part2.adoc[]
