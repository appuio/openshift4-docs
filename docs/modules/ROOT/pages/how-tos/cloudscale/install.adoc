= Installation on cloudscale.ch
:ocp-minor-version: 4.7
:ocp-patch-version: {ocp-minor-version}.0

[abstract]
--
Steps to install an OpenShift 4 cluster on https://cloudscale.ch[cloudscale.ch].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
The commands are idempotent and can be retried if any of the steps fail.

The certificates created during bootstrap are only valid for 24h.
So make sure you complete these steps within 24h.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Cloudscale

== Prerequisites

* `docker`
* `mc` https://docs.min.io/docs/minio-client-quickstart-guide.html[Minio client] (aliased to `mc` if necessary)
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)
* `openshift-install` (direct download: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-linux.tar.gz[linux], https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-mac.tar.gz[macOS])
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `qemu-img`
* `curl`
* `gzip`

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

include::partial$install/register.adoc[]

=== Configure input

.Access to various APIs
[source,console]
----
# From https://control.cloudscale.ch/user/api-tokens
export CLOUDSCALE_TOKEN=<cloudscale-api-token>

# From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_TOKEN=<gitlab-api-token>
export GITLAB_USER=<gitlab-user-name>

# For example: https://api.syn.vshn.net
# IMPORTANT: do NOT add a trailing `/`. Commands below will fail.
export COMMODORE_API_URL=<lieutenant-api-endpoint>
export COMMODORE_API_TOKEN=<lieutenant-api-token>
----

include::partial$install/vshn-input.adoc[]

=== Set up S3 bucket for cluster bootstrap

. Create S3 bucket

.. If a bucket user already exists for this cluster:
+
[source,console]
----
# Use already existing bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")
----

.. To create a new bucket user:
+
[source,console]
----
# Create a new user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  -F display_name=${CLUSTER_ID} \
  https://api.cloudscale.ch/v1/objects-users)
----

. Configure the Minio client
+
[source,console]
----
export REGION=$(curl -sH "Authorization: Bearer ${COMMODORE_API_TOKEN}" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .facts.region)
mc config host add \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

mc mb --ignore-existing \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition"
----

=== Upload Red Hat CoreOS image

. Export the Authorization header for the Cloudscale API.
+
[source,console]
----
export AUTH_HEADER="Authorization: Bearer ${CLOUDSCALE_TOKEN}"
----
+
[NOTE]
====
The variable `CLOUDSCALE_TOKEN` could be used directly.
Exporting the variable `AUTH_HEADER` is done to be compatible with the https://www.cloudscale.ch/en/api/[Cloudscale API documentation].
====

. Check if image already exists in the correct zone
+
[source,console,subs="attributes+"]
----
curl -sH "$AUTH_HEADER" https://api.cloudscale.ch/v1/custom-images | jq -r '.[] | select(.slug == "rhcos-{ocp-minor-version}") | .zones[].slug'
----
+
[NOTE]
====
If a URL is printed to the output, you can skip the next steps and directly jump to the next section.
====

. Fetch and convert the latest Red Hat CoreOS image
+
[source,console,subs="attributes+"]
----
curl -sL https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/{ocp-minor-version}/{ocp-patch-version}/rhcos-{ocp-patch-version}-x86_64-openstack.x86_64.qcow2.gz | gzip -d > rhcos-{ocp-minor-version}.gcow
qemu-img convert rhcos-{ocp-minor-version}.gcow rhcos-{ocp-minor-version}.raw
----

. Upload the image to S3 and make it public
+
[source,console,subs="attributes+"]
----
mc cp rhcos-{ocp-minor-version}.raw "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/"
mc policy set download "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.raw"
----
+
[NOTE]
====
The output of the above looks like an error.
But when checking with the following, the result is as expected.
[source,console,subs="attributes+"]
----
mc policy get "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.raw"
----
The output should be `Access permission for `[â€¦]-bootstrap-ignition/rhcos-{ocp-minor-version}.raw` is `download``

====

. Import the image to Cloudscale
+
[source,console,subs="attributes+"]
----
curl -i -H "$AUTH_HEADER" \
  -F url="$(mc share download --json "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.raw" | jq -r .url)" \
  -F name='RHCOS {ocp-minor-version}' \
  -F zones=rma1 \
  -F slug=rhcos-{ocp-minor-version} \
  -F source_format=raw \
  -F user_data_handling=pass-through \
  https://api.cloudscale.ch/v1/custom-images/import
----

=== Set secrets in Vault

.Connect with Vault
[source,console]
----
export VAULT_ADDR=https://vault-prod.syn.vshn.net
vault login -method=ldap username=<your.name>
----

.Store various secrets in Vault
[source,console]
----
# Set the cloudscale.ch access secrets
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cloudscale \
  token=${CLOUDSCALE_TOKEN} \
  s3_access_key=$(mc config host ls ${CLUSTER_ID} -json | jq -r .accessKey) \
  s3_secret_key=$(mc config host ls ${CLUSTER_ID} -json | jq -r .secretKey)

# Put LB API key in Vault
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/floaty \
  iam_key=${TF_VAR_lb_cloudscale_api_key} \
  iam_secret=${TF_VAR_lb_cloudscale_api_secret}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -
----

.Grab the LB hieradata repo token from Vault
[source,shell]
----
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq '.data.data')
export HIERADATA_REPO_USER=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.user')
export HIERADATA_REPO_TOKEN=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.token')
----

=== OpenShift Installer Setup

include::partial$install/installer-notes.adoc[]

. Prepare `install-config.yaml`
+
[source,console]
----
export INSTALLER_DIR="$(pwd)/target"
mkdir -p "${INSTALLER_DIR}"

cat > "${INSTALLER_DIR}/install-config.yaml" <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
EOF
----

. Render install manifests (this will consume the `install-config.yaml`)
+
[source,console]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create manifests
----

.. If you want to change the default "apps" domain for the cluster:
+
[source,console]
----
yq w -i "${INSTALLER_DIR}/manifests/cluster-ingress-02-config.yml" \
  spec.domain apps.example.com
----

. Render and upload ignition config (this will consume all the manifests)
+
[source,console]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create ignition-configs

mc cp "${INSTALLER_DIR}/bootstrap.ign" "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/"

export TF_VAR_ignition_bootstrap=$(mc share download \
  --json --expire=4h \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/bootstrap.ign" | jq -r '.share')
----

=== Terraform Cluster Config

[NOTE]
====
Check https://syn.tools/commodore/running-commodore.html[Running Commodore] for details on how to run commodore.
====

. Prepare Commodore inventory.
+
[source,console]
----
mkdir -p inventory/classes/
git clone $(curl -sH"Authorization: Bearer ${COMMODORE_API_TOKEN}" "${COMMODORE_API_URL}/tenants/${TENANT_ID}" | jq -r '.gitRepo.url') inventory/classes/${TENANT_ID}
----

. Prepare Terraform cluster config
+
[source,console]
----
CA_CERT=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  "${INSTALLER_DIR}/master.ign" | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)

pushd "inventory/classes/${TENANT_ID}/"

yq eval -i '.applications += ["openshift4-terraform"]' ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.infraID = \"$(jq -r .infraID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.clusterID = \"$(jq -r .clusterID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.appsDomain = \"apps.${CLUSTER_ID}.${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.base_domain = \"${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"${CA_CERT}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.hieradata_repo_user = \"${HIERADATA_REPO_USER}\"" \
  ${CLUSTER_ID}.yml

# Configure default ingress controller with 3 replicas, so that the
# VSHN-managed LB HAproxy health check isn't complaining about a missing backend
yq eval -i ".parameters.openshift4_ingress.ingressControllers.default.replicas = 3" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.vshnLdap.serviceId = \"${LDAP_ID}\"" \
  ${CLUSTER_ID}.yml

# Configure Git author information for the CI pipeline
yq eval -i ".parameters.openshift4_terraform.gitlab_ci.git.username = \"GitLab CI\"" \
  ${CLUSTER_ID}.yml
yq eval -i ".parameters.openshift4_terraform.gitlab_ci.git.email = \"tech+${CLUSTER_ID}@vshn.ch\"" \
  ${CLUSTER_ID}.yml

# Have a look at the file ${CLUSTER_ID}.yml.
# Override any default parameters or add more component configuration.

git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push Terraform setup
+
[source,console]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

=== Provision Infrastructure

. Configure Terraform secrets
+
[source,console]
----
cat <<EOF > catalog/manifests/openshift4-terraform/terraform.env
CLOUDSCALE_TOKEN
TF_VAR_ignition_bootstrap
TF_VAR_lb_cloudscale_api_key
TF_VAR_lb_cloudscale_api_secret
TF_VAR_control_vshn_net_token
GIT_AUTHOR_NAME
GIT_AUTHOR_EMAIL
HIERADATA_REPO_TOKEN
EOF
----

include::partial$setup_terraform.adoc[]

. Provision bootstrap node
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  master_count    = 0
  infra_count     = 0
  worker_count    = 0
}
EOF

terraform apply
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records` from the previous step

. Create LB hieradata
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 0
  master_count    = 0
  infra_count     = 0
  storage_count   = 0
  worker_count    = 0
}
EOF
terraform apply -target module.cluster.local_file.lb_hieradata[0]
----

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and wait until the deploy pipeline after the merge is completed.

. Create LBs
+
[source,console]
----
terraform apply
----


////
########################
TODO fix command below
########################
////
. Make LB FQDNs available for later steps
+
.Store LB FQDNs in environment
[source,shell]
----
declare -a LB_FQDNS
for id in 0 1; do
  LB_FQDNS[$id]=$(terraform state show module.cluster.exoscale_domain_record.lb[$id] | grep hostname | cut -d'=' -f2 | tr -d ' "\r\n')
done
----
+
.Verify FQDNs
[source,shell]
----
echo "${LB_FQDNS[*]}"
----

. Check LB connectivity
+
[source,shell]
----
for lb in ${LB_FQDNS[*]}; do
  ping -c1 "${lb}"
done
----

. Wait until LBs are fully initialized by Puppet
+
[source,shell]
----
# Wait for Puppet provisioning to complete
while true; do
  curl --connect-timeout 1 "http://api.${CLUSTER_ID}.${BASE_DOMAIN}:6443"
  if [ $? -eq 52 ]; then
    echo -e "\nHAproxy up"
    break
  else
    echo -n "."
    sleep 5
  fi
done
# Update sshop config, see https://wiki.vshn.net/pages/viewpage.action?pageId=40108094
sshop_update
# Check that you can access the LBs using your usual SSH config
for lb in ${LB_FQDNS[*]}; do
  ssh "${lb}" hostname -f
done
----
+
[TIP]
====
While you're waiting for the LBs to be provisioned, you can check the cloud-init logs with the following SSH commands

[source,shell]
----
ssh ubuntu@"${LB_FQDNS[0]}" tail -f /var/log/cloud-init-output.log
ssh ubuntu@"${LB_FQDNS[1]}" tail -f /var/log/cloud-init-output.log
----
====

. Check the https://ticket.vshn.net/issues/?jql=project%20%3D%20APPU%20AND%20status%20%3D%20New%20AND%20text%20~%20%22server%20created%22["Server created" tickets] for the LBs and link them to the cluster setup ticket.

include::partial$install/bootstrap-nodes.adoc[]

. Create secret with S3 credentials https://docs.openshift.com/container-platform/{ocp-minor-version}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry]
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=$(mc config host ls ${CLUSTER_ID} -json | jq -r .accessKey) \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=$(mc config host ls ${CLUSTER_ID} -json | jq -r .secretKey)
----

include::partial$install/finalize.adoc[]
