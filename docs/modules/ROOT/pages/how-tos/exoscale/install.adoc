= Installation on Exoscale
:ocp-minor-version: 4.13
:k8s-minor-version: 1.26
:ocp-patch-version: {ocp-minor-version}.0
:provider: exoscale
:kubectl_extra_args:

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

NOTE: This guide is currently assuming that you're using https://github.com/appuio/terraform-openshift4-exoscale/releases/tag/v4.0.0[`terraform-openshift4-exoscale` v4] (component https://github.com/appuio/component-openshift4-terraform/releases/tag/v6.0.0[openshift4-terraform v6])

== Starting situation

* You already have a Tenant and its Git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
+
IMPORTANT: Don't use your personal account to login to the cluster manager for installation.
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Exoscale

== Prerequisites

include::partial$install/prerequisites.adoc[]
* `md5sum`
* `virt-edit`
* `cpio`
* `exo` >= v1.71.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* An Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key] with full permissions
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

include::partial$install/register.adoc[]

=== Configure input

include::partial$exoscale/environment-vars.adoc[]

include::partial$install/vshn-input.adoc[]

[#_create_iam_keys]
=== Create restricted Exoscale IAM keys for the LBs and object storage

[NOTE]
====
If creating the API key fails, please retry the commands starting from the command which contains `exo x create-api-key`.
It may take a second or two for the newly created role to be available to reference for an API key.
====

. Create restricted API key for Exoscale object storage
+
[source,bash]
----
# Create SOS IAM role, if it doesn't exist yet in the organization
sos_iam_role_id=$(exo x list-iam-roles | \
  jq -r '."iam-roles"[] | select(.name=="sos-full-access") | .id')
if [ -z "${sos_iam_role_id}" ]; then
sos_iam_role_id=$(echo '{
  "name": "sos-full-access",
  "policy": {
    "default-service-strategy": "deny",
    "services": {
      "sos": {"type": "allow"}
    }
  }
}' | exo x  create-iam-role | jq -r '.reference.id')
fi
# Create access key
exoscale_s3_credentials=$(echo '{
  "name": "'"${CLUSTER_ID}"'_object_storage",
  "role-id": "'"${sos_iam_role_id}"'"
}' | exo x create-api-key)
export EXOSCALE_S3_ACCESSKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.key')
export EXOSCALE_S3_SECRETKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.secret')
----

. Create restricted API key for Floaty
+
[source,bash]
----
# Create Floaty IAM role if it doesn't exist yet in the organization
floaty_iam_role_id=$(exo x list-iam-roles | \
  jq -r '."iam-roles"[] | select(.name=="floaty") | .id')
if [ -z "${floaty_iam_role_id}" ]; then
cat >floaty-role.json <<EOF
{
  "name": "floaty",
  "policy": {
    "default-service-strategy": "deny",
    "services": {
      "compute-legacy": {
        "type": "rules",
        "rules": [
          {
            "action": "allow",
            "expression": "operation in ['compute-add-ip-to-nic', 'compute-list-nics', 'compute-list-resource-details', 'compute-list-virtual-machines', 'compute-query-async-job-result', 'compute-remove-ip-from-nic']"
          }
        ]
      }
    }
  }
}
EOF
floaty_iam_role_id=$(exo x create-iam-role < floaty-role.json | jq -r '.reference.id')
rm floaty-role.json
fi
# Create access key
exoscale_floaty_credentials=$(echo '{
  "name": "'"${CLUSTER_ID}"'_floaty",
  "role-id": "'"${floaty_iam_role_id}"'"
}' | exo x create-api-key)
export TF_VAR_lb_exoscale_api_key=$(echo "${exoscale_floaty_credentials}" | \
  jq -r '.key')
export TF_VAR_lb_exoscale_api_secret=$(echo "${exoscale_floaty_credentials}" | \
  jq -r '.secret')
----

. Create restricted API key for AppCat Provider Exoscale
+
[source,bash]
----
# Create AppCat Provider Exoscale IAM role, if it doesn't exist yet in the organization
appcat_role_id=$(exo x list-iam-roles | \
  jq -r '."iam-roles"[] | select(.name=="appcat-provider-exoscale") | .id')
if [ -z "${sos_iam_role_id}" ]; then
appcat_role_id=$(echo '{
  "name": "appcat-provider-exoscale",
  "policy": {
    "default-service-strategy": "deny",
    "services": {
      "sos": {"type": "allow"},
      "dbaas": {"type": "allow"},
      "iam": {"type": "allow"}
    }
  }
}' | exo x  create-iam-role | jq -r '.reference.id')
fi
# Create access key
appcat_credentials=$(echo '{
  "name": "appcat-provider-exoscale",
  "role-id": "'"${appcat_role_id}"'"
}' | exo x create-api-key)
export APPCAT_ACCESSKEY=$(echo "${appcat_credentials}" | jq -r '.key')
export APPCAT_SECRETKEY=$(echo "${appcat_credentials}" | jq -r '.secret')
----

[#_bootstrap_bucket]
=== Set up S3 bucket for cluster bootstrap

. Create S3 bucket
+
[source,bash]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap" --zone "${EXOSCALE_ZONE}"
----

[#_upload_coreos_image]
=== Upload Red Hat CoreOS image

. Fetch and convert the latest Red Hat CoreOS image
+
[source,bash,subs="attributes+"]
----
RHCOS_VERSION="{ocp-patch-version}"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/{ocp-minor-version}/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

sudo virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo compute instance-template register "rhcos-${RHCOS_VERSION}" \
  "https://${EXOSCALE_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2" \
  "$(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }')" \
  --zone "${EXOSCALE_ZONE}" \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export RHCOS_TEMPLATE="rhcos-${RHCOS_VERSION}"
----


[#_set_vault_secrets]
=== Set secrets in Vault

include::partial$connect-to-vault.adoc[]

.Store various secrets in Vault
[source,bash]
----
# Set the Exoscale object storage API key
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale/storage_iam \
  s3_access_key=${EXOSCALE_S3_ACCESSKEY} \
  s3_secret_key=${EXOSCALE_S3_SECRETKEY}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -

# Copy the VSHN acme-dns registration password
vault kv get -format=json "clusters/kv/template/cert-manager" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cert-manager" -

# Set the AppCat Provider Exoscale Credentials
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/appcat/provider-exoscale \
  access-key=${APPCAT_ACCESSKEY} \
  secret-key=${APPCAT_SECRETKEY}
----

include::partial$get-hieradata-token-from-vault.adoc[]

include::partial$install/prepare-commodore.adoc[]

[#_configure_installer]
=== Configure the OpenShift Installer

include::partial$install/configure-installer.adoc[]

[#_run_installer]
=== Run OpenShift Installer

include::partial$install/run-installer.adoc[]

. Upload ignition config
+
[source,bash]
----
exo storage upload "${INSTALLER_DIR}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read
----

=== Terraform Cluster Config

include::partial$install/prepare-syn-config.adoc[]

=== Provision Infrastructure

include::partial$exoscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]

. Provision Domain and security groups
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 0
  lb_count                 = 0
  master_count             = 0
  infra_count              = 0
  storage_count            = 0
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply
----

. Create LB hieradata
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count          = 0
  master_count             = 0
  infra_count              = 0
  storage_count            = 0
  worker_count             = 0
  additional_worker_groups = {}
}
EOF
terraform apply -target "module.cluster.module.lb.module.hiera"
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records` from the previous step

. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and wait until the deploy pipeline after the merge is completed.

. Create LBs
+
[source,bash]
----
terraform apply
----

. Make LB FQDNs available for later steps
+
.Store LB FQDNs in environment
[source,bash]
----
declare -a LB_FQDNS
for id in 1 2; do
  LB_FQDNS[$id]=$(terraform state show "module.cluster.module.lb.exoscale_domain_record.lb[$(expr $id - 1)]" | grep hostname | cut -d'=' -f2 | tr -d ' "\r\n')
done
----
+
.Verify FQDNs
[source,bash]
----
for lb in "${LB_FQDNS[@]}"; do echo $lb; done
----

include::partial$install/bootstrap-lb.adoc[]

include::partial$install/bootstrap-nodes.adoc[]

. Create secret with S3 credentials https://docs.openshift.com/container-platform/{ocp-minor-version}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry]
+
[source,bash]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${EXOSCALE_S3_ACCESSKEY} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${EXOSCALE_S3_SECRETKEY}
----
+
include::partial$install/registry-samples-operator.adoc[]

include::partial$install/finalize_part1.adoc[]

include::partial$install/finalize_part2.adoc[]

include::partial$install/post-tasks.adoc[]
