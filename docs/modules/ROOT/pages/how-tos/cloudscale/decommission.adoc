= Uninstallation on cloudscale.ch

[abstract]
--
Steps to remove an OpenShift 4 cluster from https://cloudscale.ch[cloudscale.ch].
--

[NOTE]
--
- The commands are idempotent and can be retried if any of the steps fail.
- In the future, this procedure will be mostly automated
--

== Prerequisites

include::partial$cloudscale/prerequisites.adoc[]

== Cluster Decommission

. Export the following vars
+
[source,bash]
----
export CLUSTER_ID=<lieutenant-cluster-id>
export GITLAB_TOKEN=<gitlab-api-token> # From https://git.vshn.net/-/user_settings/personal_access_tokens
export GITLAB_USER=<gitlab-user-name>
----

. Grab cluster tokens and facts from Vault and Lieutenant
+
include::partial$connect-to-vault.adoc[]
+
[source,bash]
----
export TENANT_ID=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .tenant)
export CLOUDSCALE_API_TOKEN=$(vault kv get -format=json clusters/kv/$TENANT_ID/$CLUSTER_ID/cloudscale | jq -r .data.data.token)
export REGION=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .facts.region)
export BACKUP_REGION=$(curl -H "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" https://api.cloudscale.ch/v1/regions | jq -r '.[].slug' | grep -v $REGION)
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq -r '.data.data.token')
----

. Compile the catalog for the cluster.
Having the catalog available locally enables us to run Terraform for the cluster to make any required changes.
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----

. Configure Terraform secrets
+
[source,bash]
----
cat <<EOF > ./terraform.env
CLOUDSCALE_API_TOKEN
HIERADATA_REPO_TOKEN
EOF
----

include::partial$setup_terraform.adoc[]

. Grab location of LB backups and potential Icinga2 satellite host before decommissioning VMs.
+
[source,shell]
----
declare -a LB_FQDNS
for id in 1 2; do
  LB_FQDNS[$id]=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[$(expr $id - 1)]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')
done
for lb in ${LB_FQDNS[*]}; do
  ssh "${lb}" "sudo grep 'server =' /etc/burp/burp.conf && sudo grep 'ParentZone' /etc/icinga2/constants.conf"
done
----

. Set downtimes for both LBs in https://monitoring.vshn.net[Icinga2].

. Remove APPUiO hieradata Git repository resource from Terraform state
+
[source,console]
----
terraform state rm module.cluster.module.lb.module.hiera[0].gitfile_checkout.appuio_hieradata
----
+
NOTE: This step is necessary to ensure the subsequent `terraform destroy` completes without errors.

. Delete resources from clouscale.ch using Terraform
+
[source,bash]
----
# The first time it will fail
terraform destroy
# Destroy a second time to delete private networks
terraform destroy
----
+
[source,bash]
----
popd
----

. After all resources are deleted we need to remove the buckets
+
[source,bash]
----
# Use already exiting bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")

# configure minio client to use the bucket
mc config host add \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

# delete bootstrap-ignition bucket (should already be deleted after setup)
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition" --force

# delete image-registry object
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-image-registry" --force
----

. Delete the cluster-backup bucket in the cloudscale.ch project
+
[NOTE]
====
Verify that the cluster backups aren't needed anymore before cleaning up the backup bucket.
Consider extracting the most recent cluster objects and etcd backups before deleting the bucket.
See the xref:how-tos/recover-from-backup.adoc[Recover objects from backup] how-to for instructions.
At this point in the decommissioning process, you'll have to extract the Restic configuration from Vault instead of the cluster itself.
====
+
[source,bash]
----
# configure minio client to use the bucket
mc config host add \
  "${CLUSTER_ID}_backup" "https://objects.${BACKUP_REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

mc rb "${CLUSTER_ID}_backup/${CLUSTER_ID}-cluster-backup" --force

# delete cloudscale.ch user object
curl -i -H "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" -X DELETE $(echo $response | jq -r '.href')
----

. Delete vault entries:
+
[source,bash]
----
for secret in $(find catalog/refs/ -type f -printf "clusters/kv/%P\n" \
    | sed -r 's#(.*)/.*#\1#' | grep -v '__shared__/__shared__' \
    | sort -u);
do
  vault kv delete "$secret"
done
----

. Decommission Puppet-managed LBs according to the https://wiki.vshn.net/display/VT/How+To%3A+Decommission+a+VM[VSHN documentation] (Internal link).
+
NOTE: Don't forget to remove the LB configuration in the https://git.vshn.net/appuio/appuio_hieradata/-/tree/master/lbaas[APPUiO hieradata] and the https://git.vshn.net/vshn-puppet/nodes_hieradata[nodes hieradata].

. Delete cluster from Lieutenant API (via portal)
+
Go to https://control.vshn.net/syn/lieutenantclusters
+
- Select the Lieutenant API Endpoint
+
- Search cluster name
+
- Delete cluster entry using the delete button

. Delete all remaining volumes which were associated with the cluster in the cloudscale.ch project.
+
TIP: This step is required because the csi-cloudscale driver doesn't have time to properly cleanup PVs when the cluster is decommissioned with `terraform destroy`.

. Delete the cluster-backup bucket in the cloudscale.ch project
+
[NOTE]
====
Verify that the cluster backups aren't needed anymore before cleaning up the backup bucket.
Consider extracting the most recent cluster objects and etcd backups before deleting the bucket.
See the xref:how-tos/recover-from-backup.adoc[Recover objects from backup] how-to for instructions.
At this point in the decommissioning process, you'll have to extract the Restic configuration from Vault instead of the cluster itself.
====

. Delete the cluster's API tokens in the cloudscale UI

. Delete Keycloak service (via portal)
+
Go to https://control.vshn.net/vshn/services
+
- Search cluster name
+
- Delete cluster entry service using the delete button

. Delete all DNS records related with cluster (zonefiles)

. Update any related documentation
