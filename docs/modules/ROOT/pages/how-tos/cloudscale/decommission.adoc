ifeval::["{puppet_lbs}" != "no"]
= Uninstallation on cloudscale
endif::[]

:provider: cloudscale

[abstract]
--
Steps to remove an OpenShift 4 cluster from https://cloudscale[cloudscale.ch].
--

[NOTE]
--
- The commands are idempotent and can be retried if any of the steps fail.
- In the future, this procedure will be mostly automated
--

== Prerequisites

include::partial$cloudscale/prerequisites.adoc[]
* `emergency-credentials-receive` https://github.com/vshn/emergency-credentials-receive?tab=readme-ov-file#install-from-binary[Install instructions]
* `mc` >= `RELEASE.2024-01-18T07-03-39Z` https://docs.min.io/docs/minio-client-quickstart-guide.html[Minio client] (aliased to `mc` if necessary)

== Cluster Decommission

. Setup environment variables for GitLab and Vault credentials
+
[source,bash]
----
export GITLAB_TOKEN=<gitlab-api-token> # From https://git.vshn.net/-/user_settings/personal_access_tokens
export GITLAB_USER=<gitlab-user-name>
----
+
include::partial$connect-to-vault.adoc[]

include::partial$commodore-init.adoc[]

. Setup environment variables for cloudscale credentials
+
[source,bash]
----
export CLOUDSCALE_API_TOKEN=$(vault kv get -format=json clusters/kv/$TENANT_ID/$CLUSTER_ID/cloudscale | jq -r .data.data.token)
export REGION=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .facts.region)
export BACKUP_REGION=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" https://api.cloudscale.ch/v1/regions | jq -r '.[].slug' | grep -v $REGION)
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq -r '.data.data.token')
----

include::partial$decommission-disable-syn.adoc[]

. Delete all LB services
+
[source,bash]
----
kubectl delete svc --field-selector spec.type=LoadBalancer -A
----

. Delete all loadbalancers
+
[source,bash]
----
kubectl delete loadbalancers -A --all
----

. Disable autoscaling
+
[source,bash]
----
kubectl delete machineautoscaler -A --all
----

. Delete all PVs
+
[source,bash]
----
kubectl cordon -l node-role.kubernetes.io/worker
kubectl get po -A -oyaml | yq '.items = [.items[] |
    select(.spec.nodeName | test("master-") | not) |
    select(.metadata.namespace != "syn-csi-cloudscale")]' |\
    kubectl delete --wait=false -f-
kubectl delete pvc -A --all --wait=false
kubectl wait --for=delete pv --all --timeout=120s
----
+
NOTE: By cordoning all non-master nodes and deleting all their pods (except the csi driver pods) we ensure that no new PVs are created, while the existing ones can be cleaned up.
Deleting all pods has the additional benefit that we don't have to deal with PDBs when deleting the machinesets in the next step.
+
[NOTE]
====
Delete remaining PVs explicitly if not all PVs were deleted in the previous step.

[source,bash]
----
kubectl delete pv --all
----
====

. Delete all machinesets
+
[source,bash]
----
kubectl -n openshift-machine-api delete machinesets --all
kubectl -n openshift-machine-api wait --for=delete \
    machinesets,machines --all --timeout=120s
kubectl get nodes
----

. Configure Terraform secrets
+
[source,bash,attributes="subs+"]
----
cat <<EOF > ./terraform.env
CLOUDSCALE_API_TOKEN
ifeval::["{puppet_lbs}" != "no"]
HIERADATA_REPO_TOKEN
endif::[]
EOF
----

include::partial$setup_terraform.adoc[]

ifeval::["{puppet_lbs}" != "no"]
include::partial$prepare-for-lb-decommission.adoc[]
endif::[]

. Delete resources from cloudscale using Terraform
+
[source,bash,attributes="subs+"]
----
# The first time it will fail
terraform destroy
ifeval::["${puppet_lbs}" == "no"]
# Destroy a second time to delete private networks
terraform destroy
endif::[]
----
+
[source,bash]
----
popd
----

. Delete cloudscale server groups
+
TIP: This may not fully clean up server groups for cloudscale clusters which were created earlier than 2025-10-21.
+
[source,bash]
----
for server_group_uuid in $(curl -H"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/server-groups | \
  jq --arg cluster_id "${CLUSTER_ID}" \
  '.[]|select(.name|startswith($cluster_id))|.uuid'); do
  curl -H"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
    "https://api.cloudscale.ch/v1/server-groups/${server_group_uuid}" \
    -XDELETE
done
----

ifeval::["{puppet_lbs}" == "no"]
. Delete the cloudscale subnet and private network
+
[source,bash]
----
SUBNET_UUID=$(yq '.parameters.openshift.cloudscale.subnet_uuid' \
  "inventory/classes/${TENANT_ID}/${CLUSTER_ID}.yml")
NETWORK_UUID=$(curl -sH"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  "https://api.cloudscale.ch/v1/subnets/${SUBNET_UUID}" |\
  jq -r ".network.uuid")

curl -H"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  "https://api.cloudscale.ch/v1/subnets/${SUBNET_UUID}" \
  -XDELETE
curl -H"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  "https://api.cloudscale.ch/v1/networks/${NETWORK_UUID}" \
  -XDELETE
----
endif::[]

. After all resources are deleted we need to remove the buckets
+
[source,bash]
----
# Use already exiting bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")

# configure minio client to use the bucket
mc alias set \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

# delete bootstrap-ignition bucket (should already be deleted after setup)
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition" --force

# delete image-registry bucket
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-image-registry" --force

# delete Loki logstore bucket
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-logstore" --force
----

. Delete the cluster-backup bucket in the cloudscale project
+
[NOTE]
====
Verify that the cluster backups aren't needed anymore before cleaning up the backup bucket.
Consider extracting the most recent cluster objects and etcd backups before deleting the bucket.
See the xref:how-tos/recover-from-backup.adoc[Recover objects from backup] how-to for instructions.
At this point in the decommissioning process, you'll have to extract the Restic configuration from Vault instead of the cluster itself.
====
+
[source,bash]
----
# configure minio client to use the bucket
mc alias set \
  "${CLUSTER_ID}_backup" "https://objects.${BACKUP_REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

mc rb "${CLUSTER_ID}_backup/${CLUSTER_ID}-cluster-backup" --force

# delete cloudscale object user
curl -i -H "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" -X DELETE $(echo $response | jq -r '.href')
----

. Delete all remaining volumes which were associated with the cluster in the cloudscale project.
+
TIP: This step is required because the csi-cloudscale driver doesn't have time to properly cleanup PVs when the cluster is decommissioned with `terraform destroy`.

. Delete the cluster's API tokens in the cloudscale UI

include::partial$vshn-decommission.adoc[]
