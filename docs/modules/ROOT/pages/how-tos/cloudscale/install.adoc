= Install OpenShift 4 on cloudscale.ch
:ocp-minor-version: 4.19
:k8s-minor-version: 1.32
:ocp-patch-version: {ocp-minor-version}.10
:provider: cloudscale
:needs_hieradata_edit: no

[abstract]
--
Steps to install an OpenShift 4 cluster on https://cloudscale.ch[cloudscale.ch].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
The commands are idempotent and can be retried if any of the steps fail.

The certificates created during bootstrap are only valid for 24h.
So make sure you complete these steps within 24h.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
+
IMPORTANT: Don't use your personal account to login to the cluster manager for installation.
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on cloudscale.ch

== Prerequisites

include::partial$install/prerequisites.adoc[]
* `mc` >= `RELEASE.2024-01-18T07-03-39Z` https://docs.min.io/docs/minio-client-quickstart-guide.html[Minio client] (aliased to `mc` if necessary)
* `aws` CLI https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[Official install instructions].
You can also install the Python package with your favorite package manager (we recommend https://docs.astral.sh/uv/[`uv`]: `uv tool install awscli`).
* `python3` as `python`


[WARNING]
====
Make sure the minor version of `openshift-install` and the RHCOS image are the same as ignition will fail otherwise.
====

== Cluster Installation

include::partial$install/register.adoc[]

=== Configure input

Create a new cloudscale API token with read+write permissions and name *`<cluster_id>`* on https://control.cloudscale.ch/service/<your-project>/api-token.

.Access to cloud API
[source,bash]
----
export CLOUDSCALE_API_TOKEN=<cloudscale-api-token>
----

include::partial$install/vshn-input.adoc[]

.Read cloudscale region from Project Syn cluster
[source,bash]
----
export REGION=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .facts.region)
----

=== Create private network and subnet

. Create a private network via cloudscale API
+
[source,bash]
----
response=$(curl -sH"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/networks \
  -F name="privnet_${CLUSTER_ID}" \
  -F zone="${REGION}1" \
  -F mtu=9000 \
  -F auto_create_ipv4_subnet=false)
export NETWORK_UUID=$(echo "$response" | jq -r '.uuid')
----

. Create a subnet in the private network via cloudscale API
+
[TIP]
====
Customize `PRIVNET_CIDR` if you want to use a different CIDR for the cluster.

Use a custom value for `GATEWAY_ADDR` if you don't want to use `.1` in the configured network CIDR for the default gateway.
====
+
[source,bash]
----
PRIVNET_CIDR="172.18.200.0/24"

GATEWAY_ADDR=$(python -c \
  "import ipaddress; print(next(ipaddress.ip_network(\"${PRIVNET_CIDR}\").hosts()))")

response=$(curl -sH"Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/subnets \
  -F network="${NETWORK_UUID}" \
  -F cidr="${PRIVNET_CIDR}" \
  -F gateway_address="${GATEWAY_ADDR}")
export SUBNET_UUID=$(echo "$response" | jq -r '.uuid')
----

. Create a floating IP to use as the NAT source IP via cloudscale API
+
[source,bash]
----
TBD if actually possible
----

. Ask cloudscale to provision a NAT gateway via chat.
Run the command and provide the output with your request.
+
[source]
----
cat <<EOF
---
Network UUID:     ${NETWORK_UUID}
Subnet UUID:      ${SUBNET_UUID}
Nat Gateway Name: natgw_${CLUSTER_ID}
Gateway IP:       ${GATEWAY_ADDR}
NAT source IP:    TBD if possible
---
EOF
----

[#_bootstrap_bucket]
=== Set up S3 buckets for the cluster

. Create an S3 objects user

.. If an objects user already exists for this cluster:
+
[source,bash]
----
# Use already existing bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")
----

.. To create a new objects user:
+
[source,bash]
----
# Create a new user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_API_TOKEN}" \
  -F display_name=${CLUSTER_ID} \
  https://api.cloudscale.ch/v1/objects-users)
----

. Configure the Minio client
+
[source,bash]
----
mc alias set \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')
----

. Create buckets for cluster bootstrap, the image registry and the Loki logstore
+
[source,bash]
----
mc mb --ignore-existing \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition"
mc mb --ignore-existing \
  "${CLUSTER_ID}/${CLUSTER_ID}-image-registry" <1>
# The next command isn't required for OKE!
mc mb --ignore-existing \
  "${CLUSTER_ID}/${CLUSTER_ID}-logstore" <2>
----
<1> We need to manually create the image registry bucket because we need to disable the registry operator's bucket management since the operator wants to use server-side encryption which isn't supported on cloudscale S3.
<2> (OCP only) We need to manually create the logstore bucket since the LokiStack doesn't do so automatically.

. Configure the registry bucket: Set `BlockPublicAcls=False` and setup a lifecycle policy to abort incomplete multipart uploads after a day.
+
[source,bash]
----
export AWS_ACCESS_KEY_ID=$(mc alias list ${CLUSTER_ID} -json | jq -r .accessKey)
export AWS_SECRET_ACCESS_KEY=$(mc alias list ${CLUSTER_ID} -json | jq -r .secretKey)

aws s3api put-public-access-block \
  --endpoint-url "https://objects.${REGION}.cloudscale.ch" \
  --bucket "${CLUSTER_ID}-image-registry" \
  --public-access-block-configuration BlockPublicAcls=false
aws s3api put-bucket-lifecycle-configuration \
  --endpoint-url "https://objects.${REGION}.cloudscale.ch" \
  --bucket "${CLUSTER_ID}-image-registry" \
  --lifecycle-configuration '{
    "Rules": [
      {
        "ID": "cleanup-incomplete-multipart-registry-uploads",
        "Prefix": "",
        "Status": "Enabled",
        "AbortIncompleteMultipartUpload": {
          "DaysAfterInitiation": 1
        }
      }
    ]
  }'
----

[#_upload_coreos_image]
=== Upload Red Hat CoreOS image

. Export the Authorization header for the cloudscale.ch API.
+
[source,bash]
----
export AUTH_HEADER="Authorization: Bearer ${CLOUDSCALE_API_TOKEN}"
----
+
[NOTE]
====
The variable `CLOUDSCALE_API_TOKEN` could be used directly.
Exporting the variable `AUTH_HEADER` is done to be compatible with the https://www.cloudscale.ch/en/api/[cloudscale.ch API documentation].
====

. Check if image already exists in the correct zone
+
[source,bash,subs="attributes+"]
----
curl -sH "$AUTH_HEADER" https://api.cloudscale.ch/v1/custom-images | jq -r '.[] | select(.slug == "rhcos-{ocp-minor-version}") | .zones[].slug'
----
+
[NOTE]
====
If the zone in which you're setting up the cluster is part of the list of zones returned by the command, you should skip the next steps and directly jump to the next section.
====

. Fetch the latest Red Hat CoreOS image
+
[source,bash,subs="attributes+"]
----
curl -L https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/{ocp-minor-version}/{ocp-patch-version}/rhcos-{ocp-patch-version}-x86_64-openstack.x86_64.qcow2.gz | gzip -d > rhcos-{ocp-minor-version}.qcow2
----

. Upload the image to S3 and make it public
+
[source,bash,subs="attributes+"]
----
mc cp rhcos-{ocp-minor-version}.qcow2 "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/"
mc anonymous set download "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.qcow2"
----
+
[TIP]
====
You can check that the download policy is applied successfully with

[source,bash,subs="attributes+"]
----
mc anonymous get "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.qcow2"
----

The output should be

[source,subs="attributes+"]
----
`Access permission for `[â€¦]-bootstrap-ignition/rhcos-{ocp-minor-version}.qcow2` is `download``
----
====

. Import the image to cloudscale
+
[source,bash,subs="attributes+"]
----
curl -i -H "$AUTH_HEADER" \
  -F url="$(mc share download --json "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.qcow2" | jq -r .url)" \
  -F name='RHCOS {ocp-minor-version}' \
  -F zones="${REGION}1" \
  -F slug=rhcos-{ocp-minor-version} \
  -F source_format=qcow2 \
  -F user_data_handling=pass-through \
  https://api.cloudscale.ch/v1/custom-images/import
----
+
[WARNING]
====
By default, we only import the image in the zone where we're setting up the cluster.
If you're setting up a cluster in a project where there's already a cluster using the same image slug in the other zone, you need to import the image as a "Multi-Zone: LPG1 & RMA1" image.
You can do so with the following command (note the duplicate `-F zones` flag):

[source,bash]
----
curl -i -H "$AUTH_HEADER" \
  -F url="$(mc share download --json "$\{CLUSTER_ID\}/$\{CLUSTER_ID\}-bootstrap-ignition/rhcos-{ocp-minor-version}.qcow2" | jq -r .url)" \
  -F name='RHCOS {ocp-minor-version}' \
  -F zones="lpg1" -F zones="rma1" \
  -F slug=rhcos-{ocp-minor-version} \
  -F source_format=qcow2 \
  -F user_data_handling=pass-through \
  https://api.cloudscale.ch/v1/custom-images/import
----

Otherwise, reusing an existing slug will deactivate it for any existing images in the other zone.
====

[#_set_vault_secrets]
=== Set secrets in Vault

include::partial$connect-to-vault.adoc[]

.Store various secrets in Vault
[source,bash]
----
# Set the cloudscale.ch access secrets
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cloudscale \
  token=${CLOUDSCALE_API_TOKEN} \
  s3_access_key=$(mc alias list ${CLUSTER_ID} -json | jq -r .accessKey) \
  s3_secret_key=$(mc alias list ${CLUSTER_ID} -json | jq -r .secretKey)

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)
----

include::partial$install/prepare-commodore.adoc[]

[#_configure_installer]
=== Configure the OpenShift Installer

include::partial$install/configure-installer.adoc[]

[#_run_installer]
=== Run the OpenShift Installer

include::partial$install/run-installer.adoc[]

. Upload ignition config
+
[source,bash]
----
mc cp "${INSTALLER_DIR}/bootstrap.ign" "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/"

export TF_VAR_ignition_bootstrap=$(mc share download \
  --json --expire=4h \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/bootstrap.ign" | jq -r '.share')
----

=== Terraform Cluster Config

include::partial$install/prepare-syn-config.adoc[]

=== Provision Infrastructure

include::partial$cloudscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]

include::partial$install/bootstrap-nodes.adoc[]

include::partial$install/finalize_part1.adoc[]

=== Check image registry config

Verify that the cluster image registry has been deployed correctly with the Project Syn-managed config.

. Check that no conditions show any errors in the image registry custom resource
+
[source,bash]
----
kubectl get config.imageregistry/cluster -oyaml
----

. Verify that the registry pods are running
+
[source,bash]
----
kubectl -n openshift-image-registry get pods -l docker-registry=default <1>
----
<1> This should show two pods
+
include::partial$install/registry-samples-operator.adoc[]

include::partial$install/finalize_part2.adoc[]
+
. Remove bootstrap bucket
+
[source,bash]
----
mc rm -r --force "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition"
mc rb "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition"
----

include::partial$install/post-tasks.adoc[]
