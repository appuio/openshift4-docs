= Replace a storage node

[abstract]
--
Steps to replace a storage node of an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].
--

== Starting situation

* You already have a OpenShift 4 cluster on Exoscale
* You have admin-level access to the cluster
* You want to replace an existing storage node in the cluster with a new storage node

== Prerequisites

The following CLI utilities need to be available locally:

* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `commodore`, see https://syn.tools/commodore/running-commodore.html[Running Commodore]
* `jq`
== Prepare local environment

. Create local directory to work in
+
[TIP]
====
We strongly recommend creating an empty directory, unless you already have a work directory for the cluster you're about to work on.
This guide will run Commodore in the directory created in this step.
====
+
[source,bash]
----
export WORK_DIR=/path/to/work/dir
mkdir -p "${WORK_DIR}"
pushd "${WORK_DIR}"
----

. Configure API access
+
.Access to cloud API
[source,bash]
----
export EXOSCALE_ACCOUNT=<exoscale-account>
export EXOSCALE_API_KEY=<exoscale-key>
export EXOSCALE_API_SECRET=<exoscale-secret>
export EXOSCALE_REGION=<exoscale-zone>
export EXOSCALE_S3_ENDPOINT="sos-${EXOSCALE_REGION}.exo.io"
----
+
include::partial$vshn-input.adoc[]

. Configure Exoscale CLI
+
[source,bash]
----
mkdir -p ~/.config/exoscale
cat <<EOF >> ~/.config/exoscale/exoscale.toml

[[accounts]]
  account = "${EXOSCALE_ACCOUNT}"
  defaultZone = "${EXOSCALE_REGION}"
  endpoint = "https://api.exoscale.ch/v1"
  name = "${CLUSTER_ID}"
EOF
----

. Get required tokens from Vault
+
.Connect with Vault
[source,bash]
----
export VAULT_ADDR=https://vault-prod.syn.vshn.net
vault login -method=ldap username=<your.name>
----
+
.Grab the LB hieradata repo token from Vault
[source,bash]
----
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq '.data.data')
export HIERADATA_REPO_USER=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.user')
export HIERADATA_REPO_TOKEN=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.token')
----

. Compile the catalog for the cluster.
Having the catalog available locally enables us to run Terraform for the cluster to make any required changes.
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----

== Prepare Terraform environment

. Configure Terraform secrets
+
[source,bash]
----
cat <<EOF > ./terraform.env
EXOSCALE_API_KEY
EXOSCALE_API_SECRET
TF_VAR_lb_exoscale_api_key
TF_VAR_lb_exoscale_api_secret
TF_VAR_control_vshn_net_token
GIT_AUTHOR_NAME
GIT_AUTHOR_EMAIL
HIERADATA_REPO_TOKEN
EOF
----

include::partial$setup_terraform.adoc[]

== Replace node

. Make a note of the node you want to replace
+
[source,bash]
----
export NODE_TO_REPLACE=storage-XXXX
----

=== Create a new node

. Find Terraform resource index of the node to replace
+
[source,bash]
----
# Grab JSON copy of current Terraform state
terraform state pull > .tfstate.json
node_index=$(jq --arg storage_node "${NODE_TO_REPLACE}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="random_id") |
   .instances[] |
   select(.attributes.hex==$storage_node) |
   .index_key' \
  .tfstate.json)
----

. Verify that resource index is correct
+
[source,bash]
----
jq --arg index "${node_index}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="exoscale_compute") |
   .instances[$index|tonumber] |
   .attributes.hostname' \
   .tfstate.json
----

. Remove node ID and node resource for node that we want to replace from the Terraform state
+
[source,bash]
----
terraform state rm "module.cluster.module.storage.random_id.node_id[$node_index]"
terraform state rm "module.cluster.module.storage.exoscale_compute.nodes[$node_index]"
----

. Run Terraform to spin up a replacement node
+
[source,bash]
----
terraform apply
----

. Approve node cert for new storage node
+
[source,bash]
----
# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
kubectl --as=cluster-admin get csr -w
oc --as=cluster-admin get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc --as=cluster-admin adm certificate approve

kubectl --as=cluster-admin get nodes
----

. Label and taint the new storage node
+
[source,bash]
----
kubectl --as=cluster-admin label --overwrite node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/storage=""
kubectl --as=cluster-admin label node -lnode-role.kubernetes.io/infra \
  node-role.kubernetes.io/storage-
kubectl --as=cluster-admin label node -lnode-role.kubernetes.io/app \
  node-role.kubernetes.io/storage-

kubectl --as=cluster-admin taint node -lnode-role.kubernetes.io/storage \
  storagenode=True:NoSchedule
----

. Wait for the localstorage PV on the new node to be created
+
[source,bash]
----
kubectl --as=cluster-admin get pv \
  -l storage.openshift.com/local-volume-owner-name=storagevolumes -w
----

. Disable auto sync for component `rook-ceph`.
This allows us to temporarily make manual changes to the Rook Ceph cluster.
+
[source,bash]
----
kubectl --as=cluster-admin -n syn patch apps root --type=json \
  -p '[{"op":"replace", "path":"/spec/syncPolicy", "value": {}}]'
kubectl --as=cluster-admin -n syn patch apps rook-ceph --type=json \
  -p '[{"op":"replace", "path":"/spec/syncPolicy", "value": {}}]'
----

. Make a note of the original count of OSDs in the Ceph cluster
+
[source,bash]
----
orig_osd_count=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster \
  get cephcluster cluster -o jsonpath='{.spec.storage.storageClassDeviceSets[0].count}')
----

. Change Ceph cluster to have one more OSD
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p "[{
    \"op\": \"replace\",
    \"path\": \"/spec/storage/storageClassDeviceSets/0/count\",
    \"value\": $(expr ${orig_osd_count} + 1)
  }]"
----

. Wait until the new OSD is launched
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -w
----

=== Remove old OSD

. Find the OSD on the node you want to replace
+
[source,bash]
----
OSD_ID=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  -l failure-domain="${NODE_TO_REPLACE}" --no-headers \
  -o custom-columns="NAME:.metadata.name" | cut -d- -f4)
echo $OSD_ID
----

. Verify that we found the correct OSD ID in the previous step
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -o wide | grep "osd-${OSD_ID}"
----

. Tell Ceph to take this OSD out of service and relocate data stored on it
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd out "osd.${OSD_ID}"
----

. Wait for the data to be redistributed ("backfilled") to the other OSDs.
+
TIP: When backfilling is completed, `ceph status` should show all PGs as `active+clean`.
+
NOTE: Depending on the number of OSDs in the storage cluster and the amount of data that needs to be moved, this may take a while.
+
[TIP]
====
If the storage cluster is mostly idle, you can speed up backfilling by temporarily setting the following configurations.

[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph config set osd osd_max_backfills 10 <1>
----
<1> The number of PGs which are allowed to backfill in parallel.
Adjust up or down depending on client load on the storage cluster.

After backfilling is completed, you can remove the configurations with

[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph config rm osd osd_max_backfills
----
====
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph status
----

. Remove the OSD from the Ceph cluster
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster scale --replicas=0 \
  "deploy/rook-ceph-osd-${OSD_ID}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd purge "${OSD_ID}" --yes-i-really-mean-it
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd crush remove "${NODE_TO_REPLACE}"
----

. Check that the OSD is no longer listed in `ceph osd tree`
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd tree
----

. Scale down the Rook-Ceph operator
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-operator scale --replicas=0 \
  deploy/rook-ceph-operator
----

. Check if the OSD deployment needs to be deleted, and delete it if necessary
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  -l failure-domain="${NODE_TO_REPLACE}"
# Run this command if the previous command lists a deployment
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete deploy \
  -l failure-domain="${NODE_TO_REPLACE}"
----

. Reset Ceph cluster resource to have original number of OSDs
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p "[{
    \"op\": \"replace\",
    \"path\": \"/spec/storage/storageClassDeviceSets/0/count\",
    \"value\": ${orig_osd_count}
  }]"
----

. Clean up PVC and prepare job of the old OSD if necessary
+
NOTE: We also extract the name of the PV here, but we'll only delete the PV after removing the node from the cluster.
+
[source,bash]
----
pv_info=$(kubectl get pv -l storage.openshift.com/local-volume-owner-name=storagevolumes \
  -ojson | jq --arg storage_node "${NODE_TO_REPLACE}" -r \
    '.items[]
     | select(.spec.nodeAffinity.required.nodeSelectorTerms[0].matchExpressions[0].values[0] == $storage_node)
     | "\(.metadata.name),\(.spec.claimRef.name)"')
pv_name=$(echo "${pv_info}" | cut -d, -f1)
pvc_name=$(echo "${pv_info}" | cut -d, -f2)
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete job \
  -l ceph.rook.io/pvc="${pvc_name}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete pvc "${pvc_name}"
----

. Clean up PVC encryption secret
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete secret -l pvc_name="${pvc_name}"
----

. Scale up the Rook-Ceph operator
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-operator scale --replicas=1 \
  deploy/rook-ceph-operator
----

=== Remove the old MON

. Find the MON (if any) on the node to be replaced
+
[source,bash]
----
MON_ID=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods \
  -lapp=rook-ceph-mon -o wide \
  | grep "${NODE_TO_REPLACE}" | cut -d- -f4)
echo $MON_ID
----
+
TIP: You can skip the remaining steps in this section if `$MON_ID` is empty.

. Temporarily adjust the Rook MON failover timeout.
This tells the operator to perform the MON failover after less time than the default 10 minutes.
+
[NOTE]
====
We currently have to restart the operator to force it to pick up the new MON health check configuration.
Depending on the outcome from https://github.com/rook/rook/issues/8363[this issue] on GitHub, the operator restart may not be needed in the future.
====
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p '[{
    "op": "replace",
    "path": "/spec/healthCheck/daemonHealth/mon",
    "value": {
      "disabled": false,
      "interval": "10s",
      "timeout": "10s"
    }
  }]'
kubectl --as=cluster-admin -n syn-rook-ceph-operator delete pods \
  -l app=rook-ceph-operator
----

. Wait for operator to settle.
Wait for a log message saying `done reconciling ceph cluster in namespace "syn-rook-ceph-cluster"`
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-operator logs -f \
  deploy/rook-ceph-operator
----

. Cordon node to replace and delete MON pod
+
[source,bash]
----
kubectl --as=cluster-admin cordon "${NODE_TO_REPLACE}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete pod \
  -l app=rook-ceph-mon,ceph_daemon_id="${MON_ID}"
----

. Wait until new MON is scheduled
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -w
----
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph status
----

. Verify that three MONs are running
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy -l app=rook-ceph-mon
----

. Reset the MON failover timeout
+
[NOTE]
====
We currently have to restart the operator to force it to pick up the new MON health check configuration.
Depending on the outcome from https://github.com/rook/rook/issues/8363[this issue] on GitHub, the operator restart may not be needed in the future.
====
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p '[{
    "op": "replace",
    "path": "/spec/healthCheck/daemonHealth/mon",
    "value": {}
  }]'
kubectl --as=cluster-admin -n syn-rook-ceph-operator delete pods \
  -l app=rook-ceph-operator
----

=== Clean up the old node

. Drain the node to replace
+
[source,bash]
----
kubectl --as=cluster-admin drain "${NODE_TO_REPLACE}" \
  --delete-emptydir-data --ignore-daemonsets
----

. Delete the node to replace from the cluster
+
[source,bash]
----
kubectl --as=cluster-admin delete node "${NODE_TO_REPLACE}"
----

. Find the Exoscale node id of the node to replace
+
[source,bash]
----
node_id=$(exo vm list -O json | \
  jq --arg storage_node "$NODE_TO_REPLACE" -r \
  '.[] | select(.name==$storage_node) | .id')
----

. Verify that the node ID is correct
+
[source,bash]
----
exo vm list | grep "${node_id}"
----

. Delete the node
+
[source,bash]
----
exo vm delete "${node_id}"
----

. Clean up localstorage PV of decommissioned node
+
[source,bash]
----
kubectl --as=cluster-admin delete pv "${pv_name}"
----

=== Finish up

. Re-enable ArgoCD auto sync
+
[source,bash]
----
kubectl --as=cluster-admin -n syn patch apps root --type=json \
  -p '[{
    "op":"replace",
    "path":"/spec/syncPolicy",
    "value": {"automated": {"prune": true, "selfHeal": true}}
  }]'
----

== Upstream documentation

* Rook documentation
** https://rook.io/docs/rook/v1.6/ceph-osd-mgmt.html#remove-an-osd[Remove an OSD]
** https://rook.io/docs/rook/v1.6/ceph-mon-health.html#failing-over-a-monitor[MON failover]
