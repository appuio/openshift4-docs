= Installation on Exoscale
:ocp-minor-version: 4.7
:ocp-patch-version: {ocp-minor-version}.13

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its Git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Exoscale

== Prerequisites

* An unrestricted Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key]
* `docker`
* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)
* `md5sum`
* `virt-edit`
* `cpio`
* `openshift-install` (direct download: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-linux.tar.gz[linux], https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-mac.tar.gz[macOS])
* Clone of the https://github.com/appuio/terraform-openshift4-exoscale[terraform-openshift4-exoscale] repository
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `curl`
* `gzip`

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

include::partial$install/register.adoc[]

=== Configure input

.Access to various APIs
[source,bash]
----
export EXOSCALE_ACCOUNT=<exoscale-account>
export EXOSCALE_API_KEY=<exoscale-key>
export EXOSCALE_API_SECRET=<exoscale-secret>
export EXOSCALE_REGION=<exoscale-zone>
export EXOSCALE_S3_ENDPOINT="sos-${EXOSCALE_REGION}.exo.io"

# From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_TOKEN=<gitlab-api-token>
export GITLAB_USER=<gitlab-user-name>

# For example: https://api.syn.vshn.net
# IMPORTANT: do NOT add a trailing `/`. Commands below will fail.
export COMMODORE_API_URL=<lieutenant-api-endpoint>
export COMMODORE_API_TOKEN=<lieutenant-api-token>
----

include::partial$install/vshn-input.adoc[]

=== Create restricted Exoscale IAM keys for the LBs and object storage

. Prepare Exoscale CLI
+
[source,bash]
----
mkdir -p ~/.config/exoscale
cat <<EOF >> ~/.config/exoscale/exoscale.toml

[[accounts]]
  account = "${EXOSCALE_ACCOUNT}"
  defaultZone = "${EXOSCALE_REGION}"
  endpoint = "https://api.exoscale.ch/v1"
  name = "${CLUSTER_ID}"
EOF
----

. Create restricted API keys
+
.Restricted API key for object storage
[source,bash]
----
export EXOSCALE_S3_SECRETKEY=$(exo iam apikey create "${CLUSTER_ID}_object_storage" \
  -o 'sos/*' -O json | jq -r '.secret')
export EXOSCALE_S3_ACCESSKEY=$(exo iam apikey show "${CLUSTER_ID}_object_storage" \
  -O json | jq -r '.key')
----
+
.Restricted API key for the LBs
[source,bash]
----
export TF_VAR_lb_exoscale_api_secret=$(exo iam apikey create "${CLUSTER_ID}_floaty" \
  -o 'compute/addIpToNic' \
  -o 'compute/listNics' \
  -o 'compute/listResourceDetails' \
  -o 'compute/listVirtualMachines' \
  -o 'compute/queryAsyncJobResult' \
  -o 'compute/removeIpFromNic' \
  -O json | jq -r '.secret')
export TF_VAR_lb_exoscale_api_key=$(exo iam apikey show "${CLUSTER_ID}_floaty" \
  -O json | jq -r '.key')
----

=== Set up S3 bucket for cluster bootstrap

. Create S3 bucket
+
[source,bash]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap"
----

=== Upload Red Hat CoreOS image

. Fetch and convert the latest Red Hat CoreOS image
+
[source,bash,subs="attributes+"]
----
RHCOS_VERSION="{ocp-patch-version}"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/{ocp-minor-version}/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo vm template register "rhcos-${RHCOS_VERSION}" \
  --checksum $(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }') \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}" \
  --url "https://${EXOSCALE_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export RHCOS_TEMPLATE="rhcos-${RHCOS_VERSION}"
----


=== Set secrets in Vault

.Connect with Vault
[source,bash]
----
export VAULT_ADDR=https://vault-prod.syn.vshn.net
vault login -method=ldap username=<your.name>
----

.Store various secrets in Vault
[source,bash]
----
# Set the Exoscale object storage API key
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale/storage_iam \
  s3_access_key=${EXOSCALE_S3_ACCESSKEY} \
  s3_secret_key=${EXOSCALE_S3_SECRETKEY}

# Put LB API key in Vault
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/floaty \
  iam_key=${TF_VAR_lb_exoscale_api_key} \
  iam_secret=${TF_VAR_lb_exoscale_api_secret}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -
----

.Grab the LB hieradata repo token from Vault
[source,bash]
----
export HIERADATA_REPO_SECRET=$(vault kv get \
  -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq '.data.data')
export HIERADATA_REPO_USER=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.user')
export HIERADATA_REPO_TOKEN=$(echo "${HIERADATA_REPO_SECRET}" | jq -r '.token')
----

=== OpenShift Installer Setup

include::partial$install/installer-notes.adoc[]

. Prepare SSH key
+
[NOTE]
====
We generate a unique SSH key pair for the cluster, because Exoscale doesn't support configuring multiple public keys for a VM.
====
+
[source,bash]
----
SSH_PRIVATE_KEY="$(pwd)/ssh_$CLUSTER_ID"
export SSH_PUBLIC_KEY="${SSH_PRIVATE_KEY}.pub"

ssh-keygen -C "vault@$CLUSTER_ID" -t ed25519 -f $SSH_PRIVATE_KEY -N ''

vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale/ssh \
  private_key=$(cat $SSH_PRIVATE_KEY | base64 --wrap 0)

ssh-add $SSH_PRIVATE_KEY
----

. Prepare `install-config.yaml`
+
[source,bash]
----
export INSTALLER_DIR="$(pwd)/target"
mkdir -p "${INSTALLER_DIR}"

cat > "${INSTALLER_DIR}/install-config.yaml" <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
sshKey: "$(cat $SSH_PUBLIC_KEY)"
EOF
----

. Render install manifests (this will consume the `install-config.yaml`)
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create manifests
----

.. If you want to change the default "apps" domain for the cluster:
+
[source,bash]
----
yq w -i "${INSTALLER_DIR}/manifests/cluster-ingress-02-config.yml" \
  spec.domain apps.example.com
----

. Prepare install manifests and ignition config
+
[source,bash]
----
openshift-install --dir "${INSTALLER_DIR}" \
  create ignition-configs

exo storage upload "${INSTALLER_DIR}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read
----

=== Terraform Cluster Config

[NOTE]
====
Check https://syn.tools/commodore/running-commodore.html[Running Commodore] for details on how to run commodore.
====

. Prepare Commodore inventory.
+
[source,bash]
----
mkdir -p inventory/classes/
git clone $(curl -sH"Authorization: Bearer ${COMMODORE_API_TOKEN}" "${COMMODORE_API_URL}/tenants/${TENANT_ID}" | jq -r '.gitRepo.url') inventory/classes/${TENANT_ID}
----

. Prepare Terraform cluster config
+
[source,bash]
----
CA_CERT=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  "${INSTALLER_DIR}/master.ign" | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)

pushd "inventory/classes/${TENANT_ID}/"

yq eval -i '.applications += ["openshift4-terraform"]' ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.infraID = \"$(jq -r .infraID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.clusterID = \"$(jq -r .clusterID "${INSTALLER_DIR}/metadata.json")\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift.appsDomain = \"apps.${CLUSTER_ID}.${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.base_domain = \"${BASE_DOMAIN}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"${CA_CERT}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.rhcos_template = \"${RHCOS_TEMPLATE}\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.ssh_key = \"$(cat ${SSH_PUBLIC_KEY})\"" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.openshift4_terraform.terraform_variables.hieradata_repo_user = \"${HIERADATA_REPO_USER}\"" \
  ${CLUSTER_ID}.yml

# Configure default ingress controller with 3 replicas, so that the
# VSHN-managed LB HAproxy health check isn't complaining about a missing backend
yq eval -i ".parameters.openshift4_ingress.ingressControllers.default.replicas = 3" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.vshnLdap.serviceId = \"${LDAP_ID}\"" \
  ${CLUSTER_ID}.yml

# Configure Git author information for the CI pipeline
yq eval -i ".parameters.openshift4_terraform.gitlab_ci.git.username = \"GitLab CI\"" \
  ${CLUSTER_ID}.yml
yq eval -i ".parameters.openshift4_terraform.gitlab_ci.git.email = \"tech+${CLUSTER_ID}@vshn.ch\"" \
  ${CLUSTER_ID}.yml

# Have a look at the file ${CLUSTER_ID}.yml.
# Override any default parameters or add more component configuration.

git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push Terraform setup
+
[source,bash]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

=== Provision Infrastructure

. Configure Terraform secrets
+
[source,bash]
----
cat <<EOF > catalog/manifests/openshift4-terraform/terraform.env
EXOSCALE_API_KEY
EXOSCALE_API_SECRET
TF_VAR_lb_exoscale_api_key
TF_VAR_lb_exoscale_api_secret
TF_VAR_control_vshn_net_token
GIT_AUTHOR_NAME
GIT_AUTHOR_EMAIL
HIERADATA_REPO_TOKEN
EOF
----

include::partial$setup_terraform.adoc[]

. Provision Domain and security groups
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 0
  lb_count        = 0
  master_count    = 0
  infra_count     = 0
  storage_count   = 0
  worker_count    = 0
}
EOF
terraform apply
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records` from the previous step

. Create LB hieradata
+
[source,bash]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 0
  master_count    = 0
  infra_count     = 0
  storage_count   = 0
  worker_count    = 0
}
EOF
terraform apply -target "module.cluster.local_file.lb_hieradata[0]"
----


. Review and merge the LB hieradata MR (listed in Terraform output `hieradata_mr`) and wait until the deploy pipeline after the merge is completed.

. Create LBs
+
[source,bash]
----
terraform apply
----

. Make LB FQDNs available for later steps
+
.Store LB FQDNs in environment
[source,bash]
----
declare -a LB_FQDNS
for id in 0 1; do
  LB_FQDNS[$id]=$(terraform state show module.cluster.exoscale_domain_record.lb[$id] | grep hostname | cut -d'=' -f2 | tr -d ' "\r\n')
done
----
+
.Verify FQDNs
[source,bash]
----
echo "${LB_FQDNS[*]}"
----

. Check LB connectivity
+
[source,bash]
----
for lb in ${LB_FQDNS[*]}; do
  ping -c1 "${lb}"
done
----

. Wait until LBs are fully initialized by Puppet
+
[source,bash]
----
# Wait for Puppet provisioning to complete
while true; do
  curl --connect-timeout 1 "http://api.${CLUSTER_ID}.${BASE_DOMAIN}:6443"
  if [ $? -eq 52 ]; then
    echo -e "\nHAproxy up"
    break
  else
    echo -n "."
    sleep 5
  fi
done
# Update sshop config, see https://wiki.vshn.net/pages/viewpage.action?pageId=40108094
sshop_update
# Check that you can access the LBs using your usual SSH config
for lb in ${LB_FQDNS[*]}; do
  ssh "${lb}" hostname -f
done
----
+
[TIP]
====
While you're waiting for the LBs to be provisioned, you can check the cloud-init logs with the following SSH commands

[source,bash]
----
ssh ubuntu@"${LB_FQDNS[0]}" tail -f /var/log/cloud-init-output.log
ssh ubuntu@"${LB_FQDNS[1]}" tail -f /var/log/cloud-init-output.log
----
====

. Check the https://ticket.vshn.net/issues/?jql=project%20%3D%20APPU%20AND%20status%20%3D%20New%20AND%20text%20~%20%22server%20created%22["Server created" tickets] for the LBs and link them to the cluster setup ticket.

include::partial$install/bootstrap-nodes.adoc[]

. Create secret with S3 credentials https://docs.openshift.com/container-platform/{ocp-minor-version}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry]
+
[source,bash]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${EXOSCALE_S3_ACCESSKEY} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${EXOSCALE_S3_SECRETKEY}
----

include::partial$install/finalize.adoc[]
