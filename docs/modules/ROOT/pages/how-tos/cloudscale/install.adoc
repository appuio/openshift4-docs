= Installation on cloudscale.ch

[abstract]
--
Steps to install an OpenShift 4 cluster on https://cloudscale.ch[cloudscale.ch].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Prerequisites
* cloudscale.ch API token
* `terraform`
* `mc` https://docs.min.io/docs/minio-client-quickstart-guide.html[Minio client]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor]
* `openshift-install` https://cloud.redhat.com/openshift/install/metal/user-provisioned[OpenShift Installer]
* Working local https://github.com/projectsyn/commodore[Commodore] setup


== Cluster Installation

. Register the new OpenShift 4 cluster in Lieutenant: https://control.vshn.net/syn/lieutenantclusters

. Configure input
+
[source,console]
----
export CLOUDSCALE_TOKEN=<cloudscale-api-token> # From https://control.cloudscale.ch/user/api-tokens
export GITLAB_TOKEN=<gitlab-api-token> # From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_CATALOG_PROJECT_ID=<project-id> # GitLab project ID of the catalog repo
export REGION=rma # rma or lpg
export CLUSTER_ID=<cluster-id>
export TENANT_ID=<tenant-id>
export BASE_DOMAIN=ocp4-poc.appuio-beta.ch
export PULL_SECRET=<redhat-pull-secret> # From https://cloud.redhat.com/openshift/install/pull-secret
----

. Create S3 buckets
+
[source,console]
----
# Use already exiting bucket user
response=$(curl -H "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")

# Or create a new one
response=$(curl -H "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  -F display_name=${CLUSTER_ID} \
  https://api.cloudscale.ch/v1/objects-users)

mc config host add \
  s3 "https://objects.${REGION}.cloudscale.ch" $(echo $response | jq -r '.keys[0].access_key') $(echo $response | jq -r '.keys[0].secret_key')

mc mb --ignore-existing\
  "s3/${CLUSTER_ID}-bootstrap-ignition"
----

. Prepare `install-config.yaml`
+
[source,console]
----
cd commodore/

mkdir -p target

cat > target/install-config.yaml <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
compute:
  - name: worker
    replicas: 3
controlPlane:
  name: master
  replicas: 3
networking:
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
    - 172.30.0.0/16
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
sshKey: "$(cat ~/.ssh/id_ed25519.pub)"
EOF
----

. Prepare install manifests and ignition config
+
[source,console]
----
openshift-install --dir target \
  create manifests

# Make any custom changes to the manifests
# For example to change the "apps" domain:
# yq w -i target/manifests/cluster-ingress-02-config.yml \
#   spec.domain apps.example.com

openshift-install --dir target \
  create ignition-configs

mc cp target/bootstrap.ign "s3/${CLUSTER_ID}-bootstrap-ignition"

export TF_VAR_ignition_bootstrap=$(mc share download --json --expire=1h "s3/${CLUSTER_ID}-bootstrap-ignition/bootstrap.ign" | jq -r '.share')
----

. Create Terraform cluster config
+
[source,console]
----
# This will fail
poetry run commodore catalog compile ${CLUSTER_ID}

CA_CERT=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' target/master.ign | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)

pushd "inventory/classes/${TENANT_ID}/"

yq w -i "${CLUSTER_ID}.yml" \
  "classes[+]" "components.openshift4-cloudscale"

yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.infraID -- "$(jq -r .infraID ../../../target/metadata.json)"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.clusterID -- "$(jq -r .clusterID ../../../target/metadata.json)"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.appsDomain -- "apps.${CLUSTER_ID}.${BASE_DOMAIN}"

yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift4_cloudscale.variables.base_domain -- "${BASE_DOMAIN}"
yq w -i "${CLUSTER_ID}.yml" \
  "parameters.openshift4_cloudscale.variables.ssh_keys[+]" -- "$(cat ~/.ssh/id_ed25519.pub)"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift4_cloudscale.variables.ignition_ca -- "${CA_CERT}"

yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift4_ingress.ingressControllers.default.defaultCertificate.name router-certs-snakeoil

git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile Terraform setup
+
[source,console]
----
poetry run commodore catalog compile ${CLUSTER_ID} --push
----

. Setup Terraform
+
[source,console]
----
export GITLAB_STATE_URL="https://git.vshn.net/api/v4/projects/${GITLAB_CATALOG_PROJECT_ID}/terraform/state/cluster"

pushd catalog/manifests/openshift4-cloudscale/

terraform init \
  "-backend-config=address=${GITLAB_STATE_URL}" \
  "-backend-config=lock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=unlock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=username=$(whoami)" \
  "-backend-config=password=${GITLAB_TOKEN}" \
  "-backend-config=lock_method=POST" \
  "-backend-config=unlock_method=DELETE" \
  "-backend-config=retry_wait_min=5"
----

. Provision bootstrap node
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  master_count    = 0
  infra_count     = 0
  worker_count    = 0
}
EOF

terraform apply
----

. Create the shown DNS records

. Provision master nodes
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  infra_count     = 0
  worker_count    = 0
}
EOF

terraform apply
----

. Create the remaining DNS records
+
[source,console]
----
terraform output -json | jq -r ".cluster_dns.value"
----

. Wait for bootstrap to complete
+
[source,console]
----
openshift-install --dir ../../../target \
  wait-for bootstrap-complete
----

. Remove bootstrap node and provision infra nodes
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  worker_count    = 0
}
EOF

terraform apply

export KUBECONFIG="$(pwd)/../../../target/auth/kubeconfig"

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
while sleep 3; do \
  oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve; \
done

kubectl get nodes -lnode-role.kubernetes.io/worker
kubectl label node -lnode-role.kubernetes.io/worker node-role.kubernetes.io/infra=""
----

. Wait for installation to complete
+
[source,console]
----
openshift-install --dir ../../../target \
  wait-for install-complete
----

. Provision worker nodes
+
[source,console]
----
rm override.tf

terraform apply

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
while sleep 3; do \
  oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve; \
done

kubectl label --overwrite node -lnode-role.kubernetes.io/worker node-role.kubernetes.io/app=""
kubectl label node -lnode-role.kubernetes.io/infra node-role.kubernetes.io/app-
----

. Create secret with S3 credentials https://docs.openshift.com/container-platform/4.5/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry] (will be https://ticket.vshn.net/browse/APPU-2790[automated])
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=$(mc config host ls s3 -json | jq -r .accessKey) \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=$(mc config host ls s3 -json | jq -r .secretKey)
----

. Create wildcard cert for router
+
[source,console]
----
kubectl get secret router-certs-default \
  -n openshift-ingress \
  -ojson --export | \
    jq 'del(.metadata.ownerReferences) | .metadata.name = "router-certs-snakeoil"' | \
  kubectl -n openshift-ingress apply -f -
----

. Make the cluster Project Syn enabled
+
Install Steward on the cluster according to https://wiki.vshn.net/x/ngMBCg
+
[source,console]
----
cat ${CLUSTER_ID}/metadata.json
----
