= Installation on Exoscale

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Starting situation

* You already have a Tenant and its git repository
* You have a CCSP Red Hat login and are logged into https://cloud.redhat.com/openshift/install/metal/user-provisioned[Red Hat Openshift Cluster Manager]
* You want to register a new cluster in Lieutenant and are about to install Openshift 4 on Cloudscale

== Prerequisites

* Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key]
* `docker`
* `terraform`
* `exo` >= v1.14.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)
* `md5sum`
* `virt-edit`
* `cpio`
* `openshift-install` (direct download: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-linux.tar.gz[linux], https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-{ocp-minor-version}/openshift-install-mac.tar.gz[macOS])
* Clone of the https://github.com/appuio/terraform-openshift4-exoscale[terraform-openshift4-exoscale] repository
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `curl`
* `gzip`
* A copy of the Ursula Debian package

[WARNING]
====
Make sure the version of openshift-install and the rhcos image is the same, otherwise ignition will fail.
====

== Cluster Installation

Register the new OpenShift 4 cluster in Lieutenant.

.Lietenant API endpoint
****
Use the following endpoint for Lieutenant:

VSHN:: https://api.syn.vshn.net
****

=== Set up LDAP service

. Create an LDAP service
+
Use https://control.vshn.net/vshn/services/_create to create a service.
The name must contain the customer and the cluster name.
And then put the LDAP service ID in the following variable:
+
[source,console]
----
export LDAP_ID="Your_LDAP_ID_here"
export LDAP_PASSWORD="Your_LDAP_pw_here"
----

=== Configure input

.Access to various APIs
[source,console]
----
export EXOSCALE_ACCOUNT=<exoscale-account>
export EXOSCALE_API_KEY=<exoscale-key>
export EXOSCALE_API_SECRET=<exoscale-secret>
export EXOSCALE_REGION=<exoscale-zone>

# From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_TOKEN=<gitlab-api-token>

# For example: https://api.syn.vshn.net
# IMPORTANT: do NOT add a trailing `/`. Commands below will fail.
export COMMODORE_API_URL=<lieutenant-api-endpoint>
# See https://wiki.vshn.net/x/ngMBCg#ClusterRegistryinLieutenantSynthesizeaCluster(synfection)-Preparation
export COMMODORE_API_TOKEN=<lieutenant-api-token>
----

.VSHN sepcific input
[source,console]
----
export CLUSTER_ID=<lieutenant-cluster-id> # Looks like: c-<something>
export TENANT_ID=$(curl -sH "Authorization: Bearer ${COMMODORE_API_TOKEN}" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .tenant)
----

.OpenShift configuration
[source,console]
----
export BASE_DOMAIN=<your-base-domain>
export PULL_SECRET='<redhat-pull-secret>' # As copied from https://cloud.redhat.com/openshift/install/pull-secret "Copy pull secret". value must be inside quotes.
----

For `BASE_DOMAIN` explanation, see xref:explanations/dns_scheme.adoc[DNS Scheme].

.Exoscale configuration
[source,console]
----
export AWS_ACCESS_KEY_ID=${EXOSCALE_API_KEY}
export AWS_SECRET_ACCESS_KEY=${EXOSCALE_API_SECRET}
export AWS_S3_ENDPOINT="sos-${EXOSCALE_REGION}.exo.io"

export SSH_PUBLIC_KEY=~/.ssh/id_ed25519.pub
export SSH_PUBKEY_FINGERPRINT=$(ssh-keygen -E md5 -lf $SSH_PUBLIC_KEY | cut -d' ' -f2 | tail -c+5)

# TODO: generate separate restricted IAM key for Ursula
export TF_VAR_lb_exoscale_api_key=${EXOSCALE_API_KEY}
export TF_VAR_lb_exoscale_api_secret=${EXOSCALE_API_SECRET}
----

=== Set up S3 bucket

. Prepare Exoscale CLI
+
[source,console]
----
mkdir -p ~/.config/exoscale
cat <<EOF >> ~/.config/exoscale/exoscale.toml

[[accounts]]
  account = "${EXOSCALE_ACCOUNT}"
  defaultZone = "${EXOSCALE_REGION}"
  endpoint = "https://api.exoscale.ch/v1"
  name = "${CLUSTER_ID}"
EOF

----

. Create S3 buckets
+
[source,console]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap"
exo storage create "sos://${CLUSTER_ID}-tf-state"

export TF_VAR_bootstrap_bucket="https://sos-${EXOSCALE_REGION}.exo.io/${CLUSTER_ID}-bootstrap/"
----

. Prepare `install-config.yaml`
+
[source,console]
----
mkdir ${CLUSTER_ID}

cat > "${CLUSTER_ID}/install-config.yaml" <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
sshKey: "$(cat $SSH_PUBLIC_KEY)"
EOF

----

. Prepare install manifests and ignition config
+
[source,console]
----
openshift-install --dir ${CLUSTER_ID} \
  create manifests

openshift-install --dir ${CLUSTER_ID} \
  create ignition-configs

exo storage upload "${CLUSTER_ID}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read

export TF_VAR_ignition_ca=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  ${CLUSTER_ID}/master.ign | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)
----

. Prepare RHCOS template
+
[source,console]
----
RHCOS_VERSION="4.7.7"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo vm template register "rhcos-${RHCOS_VERSION}" \
  --checksum $(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }') \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}" \
  --url "https://${AWS_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export TF_VAR_rhcos_template="rhcos-${RHCOS_VERSION}"
----

. Upload Ursula Debian package to bootstrap bucket
+
[source,console]
----
exo storage upload /path/to/ursula.deb "sos://${CLUSTER_ID}-bootstrap/ursula.deb" --acl public-read
----


=== Set secrets in Vault

.Connect with Vault
[source,console]
----
export VAULT_ADDR=https://vault-prod.syn.vshn.net
vault login -method=ldap username=<your.name>
----

.Store various secrets in vault
[source,console]
----
# Set the exoscale.ch access secrets
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/exoscale \
  api_key=${EXOSCALE_API_KEY} \
  api_secret=${EXOSCALE_API_SECRET} \
  s3_access_key=${EXOSCALE_API_KEY} \
  s3_secret_key=${EXOSCALE_API_SECRET}

# Generate an HTTP secret for the registry
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/registry \
  httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

# Set the LDAP password
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/vshn-ldap \
  bindPassword=${LDAP_PASSWORD}

# Generate a master password for K8up backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/global-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Generate a password for the cluster object backups
vault kv put clusters/kv/${TENANT_ID}/${CLUSTER_ID}/cluster-backup \
  password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

# Copy the Dagobert OpenShift Node Collector Credentials
vault kv get -format=json "clusters/kv/template/dagobert" | jq '.data.data' \
  | vault kv put -cas=0 "clusters/kv/${TENANT_ID}/${CLUSTER_ID}/dagobert" -
----


. Initialize Terraform
+
[NOTE]
====
Exoscale doesn't allow uploading the same public key multiple times in a single organisation.
If the public key in `$SSH_PUBLIC_KEY` already exists in the organisation, we instruct Terraform to use the existing keypair instead of trying to upload the public key under a new name.
====
+
[source,console]
----
cat > backend.tf <<EOF
terraform {
  backend "s3" {
    key                         = "cluster.tfstate"
    region                      = "us-east-1" # Ignored
    bucket                      = "${CLUSTER_ID}-tf-state"
    skip_credentials_validation = true
    skip_metadata_api_check     = true
  }
}
EOF
terraform init

export TF_VAR_cluster_id=$CLUSTER_ID
export TF_VAR_base_domain=${BASE_DOMAIN}
export TF_VAR_ssh_key="$(cut -d' ' -f1,2 <$SSH_PUBLIC_KEY)"
export TF_VAR_existing_keypair=$(exo sshkey list -Ojson | \
        jq -r ".[] | select(.fingerprint == \"${SSH_PUBKEY_FINGERPRINT}\") | .name")
----

. Create Exoscale domain
+
[source,console]
----
terraform apply -target data.exoscale_domain_record.exo_nameservers
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records`

. Deploy LBs and bootstrap node
+
[NOTE]
====
We initially provision VMs in "Stopped" state to ensure the private network interface is attached when the VM powers up.
We provision the VMs in this two-step process to avoid VMs getting stuck in a state where they're unable to reach the machine config server when deploying the cluster into a private network.
====
+
[NOTE]
====
We currently need two runs of `terraform apply -var X_state=Running` when provisioning VMs.
This is necessary because the Exoscale Terraform provider doesn't correctly signal that a VM's IP can change during a state change.
See the issue filed against the https://github.com/exoscale/terraform-provider-exoscale/issues/101[Exoscale Terraform provider] for a more detailed explanation.
====
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply \
    -var bootstrap_count=1 \
    -var master_count=0 \
    -var worker_count=0 \
    -var bootstrap_state=${state}
done
----

. Wait for bootstrap API to come up
+
[source,console]
----
API_URL=$(yq e '.clusters[0].cluster.server' ${CLUSTER_ID}/auth/kubeconfig)
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply \
    -var bootstrap_count=1 \
    -var worker_count=0 \
    -var master_state=${state}
done
----

. Wait for bootstrap to complete
+
[source,console]
----
openshift-install --dir ${CLUSTER_ID} \
  wait-for bootstrap-complete
----

. Remove bootstrap node and provision worker nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply -var worker_state=${state}
done
----

. Approve worker certs
+
[source,console]
----
export KUBECONFIG=${CLUSTER_ID}/auth/kubeconfig

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
kubectl get csr -w
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve

kubectl get nodes

openshift-install --dir ${CLUSTER_ID} \
  wait-for install-complete
----

. Create secret with S3 credentials https://docs.openshift.com/container-platform/4.7/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry] (will be https://ticket.vshn.net/browse/APPU-2790[automated])
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${AWS_ACCESS_KEY_ID} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${AWS_SECRET_ACCESS_KEY}
----

. Create wildcard cert for router
+
[source,console]
----
kubectl get secret router-certs-default \
  -n openshift-ingress \
  -ojson --export | \
    jq 'del(.metadata.ownerReferences) | .metadata.name = "router-certs-snakeoil"' | \
  kubectl -n openshift-ingress apply -f -
----

. Make the cluster Project Syn enabled
+
Install Steward on the cluster according to https://wiki.vshn.net/x/ngMBCg
+
[source,console]
----
cat ${CLUSTER_ID}/metadata.json
----
