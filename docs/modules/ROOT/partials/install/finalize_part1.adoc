ifeval::["{provider}" != "exoscale"]
ifeval::["{provider}" != "stackit"]
:acme-dns-update-zone: yes
endif::[]
endif::[]

:dummy:
ifeval::["{provider}" == "vsphere"]
=== Set default storage class

. Set storage class `thin-csi` as default
+
[source,bash]
----
kubectl annotate storageclass thin storageclass.kubernetes.io/is-default-class-
kubectl annotate storageclass thin-csi storageclass.kubernetes.io/is-default-class=true
----

endif::[]

=== Enable Project Syn

. https://kb.vshn.ch/vshnsyn/how-tos/synthesize.html[Make the cluster Project Syn enabled]

[IMPORTANT]
====
If the environment has an outgoing proxy you need to patch the steward deployment with the following config:
[source,bash]
----
env:
- name: HTTPS_PROXY
  value: http://proxy.example.com
- name: HTTP_PROXY
  value: http://proxy.example.com
- name: NO_PROXY
  value: <NO_PROXY_VALUES>
- name: http_proxy
  value: http://proxy.example.com
- name: https_proxy
  value: http://proxy.example.com
- name: no_proxy
  value: <NO_PROXY_VALUES>
----
====

=== Setup acme-dns CNAME records for the cluster

NOTE: You can skip this section if you're not using Let's Encrypt for the cluster's API and default wildcard certificates.

. Extract the acme-dns subdomain for the cluster after `cert-manager` has been deployed via Project Syn.
+
[source,bash]
----
fulldomain=$(kubectl -n syn-cert-manager \
  get secret acme-dns-client \
  -o jsonpath='{.data.acmedns\.json}' | \
  base64 -d  | \
  jq -r '[.[]][0].fulldomain')
echo "$fulldomain"
----
+
NOTE: If the `acme-dns-client` secret hasn't yet been populated, re-trigger the ArgoCD sync for `cert-manager`.

ifeval::["{acme-dns-update-zone}" == "yes"]
. Add the following CNAME records to the cluster's DNS zone
+
[IMPORTANT]
====
The `_acme-challenge` records must be created in the same zone as the cluster's `api` and `apps` records respectively.
====
+
[source,dns]
----
$ORIGIN <cluster-zone> <2>
_acme-challenge.api  IN CNAME <fulldomain>. <1>
$ORIGIN <apps-base-domain> <3>
_acme-challenge.apps IN CNAME <fulldomain>. <1>
----
<1> Replace `<fulldomain>` with the output of the previous step.
<2> The `_acme-challenge.api` record must be created in the same origin as the `api` record.
<3> The `_acme-challenge.apps` record must be created in the same origin as the `apps` record.
endif::[]
ifeval::["{provider}" == "exoscale"]
. Setup the `_acme-challenge` CNAME records in the cluster's DNS zone
+
[IMPORTANT]
====
The `_acme-challenge` records must be created in the same zone as the cluster's `api` and `apps` records respectively.
The snippet below assumes that the cluster is configured to use the default "apps" domain in the cluster's zone.
====
+
[source,bash]
----
for cname in "api" "apps"; do
  exo dns add CNAME "${CLUSTER_DOMAIN}" -n "_acme-challenge.${cname}" -a "${fulldomain}." -t 600
done
----
endif::[]
ifeval::["{provider}" == "stackit"]
. Setup the `_acme-challenge` CNAME records in the cluster's DNS zone
+
[IMPORTANT]
====
The `_acme-challenge` records must be created in the same zone as the cluster's `api` and `apps` records respectively.
The snippet below assumes that the cluster is configured to use the default "apps" domain in the cluster's zone.
====
+
[source,bash]
----
zone_id="`stackit dns zone list -ojson  | jq -r '.[] | select(.dnsName == "'"${CLUSTER_DOMAIN}"'") | .id'`"

for cname in "api" "apps"; do
  stackit dns record-set create --zone-id "$zone_id" --ttl 600 --type CNAME --name "_acme-challenge.${cname}" --record "${fulldomain}."
done
----
endif::[]

=== Ensure emergency admin access to the cluster

. Check that emergency credentials were uploaded and are accessible:
+
[source,bash]
----
export EMR_KUBERNETES_ENDPOINT=https://api.${CLUSTER_DOMAIN}:6443
emergency-credentials-receive "${CLUSTER_ID}"
----
+
[NOTE]
====
You need to be in the passbolt group `VSHN On-Call`.

If the command fails, check if the controller is already deployed, running, and if the credentials are uploaded:

[source,bash]
----
kubectl -n appuio-emergency-credentials-controller get emergencyaccounts.cluster.appuio.io -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.lastTokenCreationTimestamp}{"\n"}{end}'
----
====

. Follow the instructions from `emergency-credentials-receive` to use the downloaded `kubeconfig` file.
+
[source,bash]
----
export KUBECONFIG="em-${CLUSTER_ID}"
kubectl get nodes
oc whoami # should output system:serviceaccount:appuio-emergency-credentials-controller:*
----
+
[NOTE]
====
If the Let's Encrypt certificate for the API isn't fully provisioned yet, you may need to run the following `yq` to use the emergency kubeconfig:

[source,bash]
----
yq -i e '.clusters[0].cluster.insecure-skip-tls-verify = true' "em-${CLUSTER_ID}"
----
====

. Invalidate the 10 year admin kubeconfig.
+
[source,bash]
----
kubectl -n openshift-config patch cm admin-kubeconfig-client-ca --type=merge -p '{"data": {"ca-bundle.crt": ""}}'
----

=== Enable Opsgenie alerting

. Create the standard silence for alerts that don't have the `syn` label
+
[source,bash]
----
oc --as=system:admin -n openshift-monitoring create job --from=cronjob/silence silence-manual
oc wait -n openshift-monitoring --for=condition=complete job/silence-manual
oc --as=system:admin -n openshift-monitoring delete job/silence-manual
----

. Check the remaining active alerts and address them where neccessary
+
[source,bash]
----
kubectl --as=system:admin -n openshift-monitoring exec sts/alertmanager-main -- \
    amtool --alertmanager.url=http://localhost:9093 alert --active
----

. Remove the "no-opsgenie" class from the cluster's configuration
+
[source,bash]
----
pushd "inventory/classes/${TENANT_ID}/"
yq eval -i 'del(.classes[] | select(. == "*.no-opsgenie"))' ${CLUSTER_ID}.yml
git commit -a -m "Enable opsgenie alerting on cluster ${CLUSTER_ID}"
git push
popd
----
