= Installation on Exoscale

[abstract]
--
Steps to install an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--

== Prerequisites
* Exoscale https://community.exoscale.com/documentation/iam/quick-start/#api-keys[API key]
* `terraform`
* `exo` >= v1.14.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `jq`
* `md5sum`
* `virt-edit`
* `cpio`
* Clone of the https://github.com/appuio/terraform-openshift4-exoscale[terraform-openshift4-exoscale] repository
* https://community.exoscale.com/documentation/dns/quick-start/#subscribing-to-the-service[DNS subscription] activated in the Exoscale organisation
* A copy of the Ursula Debian package

== Cluster Installation

. Register the new OpenShift 4 cluster in Lieutenant: https://control.vshn.net/syn/lieutenantclusters

. Configure input
+
[source,console]
----
export EXOSCALE_ACCOUNT=<exoscale-account>
export EXOSCALE_API_KEY=<exoscale-key>
export EXOSCALE_API_SECRET=<exoscale-secret>
export EXOSCALE_REGION=ch-dk-2
export CLUSTER_ID=<cluster-name>
export BASE_DOMAIN=<cluster-base-domain>
export PULL_SECRET=<redhat-pull-secret> # From https://cloud.redhat.com/openshift/install/pull-secret

export AWS_ACCESS_KEY_ID=${EXOSCALE_API_KEY}
export AWS_SECRET_ACCESS_KEY=${EXOSCALE_API_SECRET}
export AWS_S3_ENDPOINT="sos-${EXOSCALE_REGION}.exo.io"

export SSH_PUBLIC_KEY=~/.ssh/id_ed25519.pub
export SSH_PUBKEY_FINGERPRINT=$(ssh-keygen -E md5 -lf $SSH_PUBLIC_KEY | cut -d' ' -f2 | tail -c+5)

# TODO: generate separate restricted IAM key for Ursula
export TF_VAR_lb_exoscale_api_key=${EXOSCALE_API_KEY}
export TF_VAR_lb_exoscale_api_secret=${EXOSCALE_API_SECRET}
----

. Prepare Exoscale CLI
+
[source,console]
----
mkdir -p ~/.config/exoscale
cat <<EOF >> ~/.config/exoscale/exoscale.toml

[[accounts]]
  account = "${EXOSCALE_ACCOUNT}"
  defaultZone = "${EXOSCALE_REGION}"
  endpoint = "https://api.exoscale.ch/v1"
  name = "${CLUSTER_ID}"
EOF

----

. Create S3 buckets
+
[source,console]
----
exo storage create "sos://${CLUSTER_ID}-bootstrap"
exo storage create "sos://${CLUSTER_ID}-tf-state"

export TF_VAR_bootstrap_bucket="https://sos-${EXOSCALE_REGION}.exo.io/${CLUSTER_ID}-bootstrap/"
----

. Prepare `install-config.yaml`
+
[source,console]
----
mkdir ${CLUSTER_ID}

cat > "${CLUSTER_ID}/install-config.yaml" <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
sshKey: "$(cat $SSH_PUBLIC_KEY)"
EOF

----

. Prepare install manifests and ignition config
+
[source,console]
----
openshift-install --dir ${CLUSTER_ID} \
  create manifests

openshift-install --dir ${CLUSTER_ID} \
  create ignition-configs

exo storage upload "${CLUSTER_ID}/bootstrap.ign" "sos://${CLUSTER_ID}-bootstrap" --acl public-read

export TF_VAR_ignition_ca=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  ${CLUSTER_ID}/master.ign | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)
----

. Prepare RHCOS template
+
[source,console]
----
RHCOS_VERSION="4.7.7"

curl "https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.7/${RHCOS_VERSION}/rhcos-${RHCOS_VERSION}-x86_64-openstack.x86_64.qcow2.gz" | gunzip > rhcos-${RHCOS_VERSION}.qcow2

virt-edit -a rhcos-${RHCOS_VERSION}.qcow2 \
  -m /dev/sda3:/ /loader/entries/ostree-1-rhcos.conf \
  -e 's/openstack/exoscale/'

exo storage upload rhcos-${RHCOS_VERSION}.qcow2 "sos://${CLUSTER_ID}-bootstrap" --acl public-read

exo vm template register "rhcos-${RHCOS_VERSION}" \
  --checksum $(md5sum rhcos-${RHCOS_VERSION}.qcow2 | awk '{ print $1 }') \
  --boot-mode uefi \
  --disable-password \
  --username core \
  --description "Red Hat Enterprise Linux CoreOS (RHCOS) ${RHCOS_VERSION}" \
  --url "https://${AWS_S3_ENDPOINT}/${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

exo storage delete "sos://${CLUSTER_ID}-bootstrap/rhcos-${RHCOS_VERSION}.qcow2"

export TF_VAR_rhcos_template="rhcos-${RHCOS_VERSION}"
----

. Upload Ursula Debian package to bootstrap bucket
+
[source,console]
----
exo storage upload /path/to/ursula.deb "sos://${CLUSTER_ID}-bootstrap/ursula.deb" --acl public-read
----

. Initialize Terraform
+
[NOTE]
====
Exoscale doesn't allow uploading the same public key multiple times in a single organisation.
If the public key in `$SSH_PUBLIC_KEY` already exists in the organisation, we instruct Terraform to use the existing keypair instead of trying to upload the public key under a new name.
====
+
[source,console]
----
cat > backend.tf <<EOF
terraform {
  backend "s3" {
    key                         = "cluster.tfstate"
    region                      = "us-east-1" # Ignored
    bucket                      = "${CLUSTER_ID}-tf-state"
    skip_credentials_validation = true
    skip_metadata_api_check     = true
  }
}
EOF
terraform init

export TF_VAR_cluster_id=$CLUSTER_ID
export TF_VAR_base_domain=${BASE_DOMAIN}
export TF_VAR_ssh_key="$(cut -d' ' -f1,2 <$SSH_PUBLIC_KEY)"
export TF_VAR_existing_keypair=$(exo sshkey list -Ojson | \
        jq -r ".[] | select(.fingerprint == \"${SSH_PUBKEY_FINGERPRINT}\") | .name")
----

. Create Exoscale domain
+
[source,console]
----
terraform apply -target data.exoscale_domain_record.exo_nameservers
----

. Set up DNS NS records on parent zone using the data from the Terraform output variable `ns_records`

. Deploy LBs and bootstrap node
+
[NOTE]
====
We initially provision VMs in "Stopped" state to ensure the private network interface is attached when the VM powers up.
We provision the VMs in this two-step process to avoid VMs getting stuck in a state where they're unable to reach the machine config server when deploying the cluster into a private network.
====
+
[NOTE]
====
We currently need two runs of `terraform apply -var X_state=Running` when provisioning VMs.
This is necessary because the Exoscale Terraform provider doesn't correctly signal that a VM's IP can change during a state change.
See the issue filed against the https://github.com/exoscale/terraform-provider-exoscale/issues/101[Exoscale Terraform provider] for a more detailed explanation.
====
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply \
    -var bootstrap_count=1 \
    -var master_count=0 \
    -var worker_count=0 \
    -var bootstrap_state=${state}
done
----

. Wait for bootstrap API to come up
+
[source,console]
----
API_URL=$(yq e '.clusters[0].cluster.server' ${CLUSTER_ID}/auth/kubeconfig)
while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
  echo -n "."
  sleep 5
done && echo -e "\nAPI is up"
----

. Deploy control plane nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply \
    -var bootstrap_count=1 \
    -var worker_count=0 \
    -var master_state=${state}
done
----

. Wait for bootstrap to complete
+
[source,console]
----
openshift-install --dir ${CLUSTER_ID} \
  wait-for bootstrap-complete
----

. Remove bootstrap node and provision worker nodes
+
[source,console]
----
for state in "Stopped" "Running" "Running"; do
  terraform apply -var worker_state=${state}
done
----

. Approve worker certs
+
[source,console]
----
export KUBECONFIG=${CLUSTER_ID}/auth/kubeconfig

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
kubectl get csr -w
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve

kubectl get nodes

openshift-install --dir ${CLUSTER_ID} \
  wait-for install-complete
----

. Create secret with S3 credentials https://docs.openshift.com/container-platform/4.7/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry] (will be https://ticket.vshn.net/browse/APPU-2790[automated])
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${AWS_ACCESS_KEY_ID} \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${AWS_SECRET_ACCESS_KEY}
----

. Create wildcard cert for router
+
[source,console]
----
kubectl get secret router-certs-default \
  -n openshift-ingress \
  -ojson --export | \
    jq 'del(.metadata.ownerReferences) | .metadata.name = "router-certs-snakeoil"' | \
  kubectl -n openshift-ingress apply -f -
----

. Make the cluster Project Syn enabled
+
Install Steward on the cluster according to https://wiki.vshn.net/x/ngMBCg
+
[source,console]
----
cat ${CLUSTER_ID}/metadata.json
----
