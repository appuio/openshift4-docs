= Remove a storage node

:kubectl_extra_args: --as=cluster-admin
:delabel_app_nodes: yes
:argo_app: rook-ceph

[abstract]
--
Steps to remove a storage node of an OpenShift 4 cluster on https://www.exoscale.com[Exoscale].
--

== Starting situation

* You already have a OpenShift 4 cluster on Exoscale
* You have admin-level access to the cluster
* You want to remove an existing storage node in the cluster

== Prerequisites

The following CLI utilities need to be available locally:

* `docker`
* `curl`
* `kubectl`
* `oc`
* `exo` >= v1.28.0 https://community.exoscale.com/documentation/tools/exoscale-command-line-interface[Exoscale CLI]
* `vault` https://www.vaultproject.io/docs/commands[Vault CLI]
* `commodore`, see https://syn.tools/commodore/running-commodore.html[Running Commodore]
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor] (version 4 or higher)

== Prepare local environment

. Create local directory to work in
+
[TIP]
====
We strongly recommend creating an empty directory, unless you already have a work directory for the cluster you're about to work on.
This guide will run Commodore in the directory created in this step.
====
+
[source,bash]
----
export WORK_DIR=/path/to/work/dir
mkdir -p "${WORK_DIR}"
pushd "${WORK_DIR}"
----

. Configure API access
+
include::partial$exoscale/environment-vars.adoc[]
+
include::partial$vshn-input.adoc[]

. Get required tokens from Vault
+
include::partial$connect-to-vault.adoc[]
+
include::partial$get-hieradata-token-from-vault.adoc[]

. Compile the catalog for the cluster.
Having the catalog available locally enables us to run Terraform for the cluster to make any required changes.
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----

== Set alert silence

:duration: +60 minutes
include::partial$create-alertmanager-silence.adoc[]

== Update Cluster Config

. Update cluster config. 
+
[source,bash]
----
pushd "inventory/classes/${TENANT_ID}/"

yq eval -i ".parameters.openshift4_terraform.terraform_variables.storage_count -= 1" \
  ${CLUSTER_ID}.yml

yq eval -i ".parameters.rook_ceph.ceph_cluster.node_count -= 1" \
  ${CLUSTER_ID}.yml

----
+
[NOTE]
====
Ceph can't scale below 3 storage nodes, which is the default number of nodes.
Please check that this update doesn't reduce the number of storage nodes below 3, before continuing.
====

. Review and commit
+
[source,bash]
----

# Have a look at the file ${CLUSTER_ID}.yml.

git commit -a -m "Remove storage node to cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push cluster catalog 
+
[source,bash]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

== Prepare Terraform environment

include::partial$exoscale/configure-terraform-secrets.adoc[]

include::partial$setup_terraform.adoc[]

== Remove Node
Make a note of the node you want to remove

[source,bash]
----
export NODE_TO_REMOVE=storage-XXXX
----

=== Remove old OSD
. Make sure ArgoCD ran and reduced the target number of OSDs.
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster \
  get cephcluster cluster -o jsonpath='{.spec.storage.storageClassDeviceSets[0].count}'
----

. Find the OSD on the node you want to replace
+
[source,bash]
----
OSD_ID=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  -l failure-domain="${NODE_TO_REMOVE}" --no-headers \
  -o custom-columns="NAME:.metadata.name" | cut -d- -f4)
echo $OSD_ID
----

. Verify that we found the correct OSD ID in the previous step
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -o wide | grep "osd-${OSD_ID}"
----

. Tell Ceph to take this OSD out of service and relocate data stored on it
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd out "osd.${OSD_ID}"
----

. Wait for the data to be redistributed ("backfilled") to the other OSDs.
+
include::partial$storage-ceph-backfilling.adoc[]

. Remove the OSD from the Ceph cluster
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster scale --replicas=0 \
  "deploy/rook-ceph-osd-${OSD_ID}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd purge "${OSD_ID}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd crush remove "${NODE_TO_REMOVE}"
----

. Check that the OSD is no longer listed in `ceph osd tree`
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph osd tree
----

. Make a note of the PVC of the old OSD
+
NOTE: We also extract the name of the PV here, but we'll only delete the PV after removing the node from the cluster.
+
[source,bash]
----
pvc_name=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  "rook-ceph-osd-${OSD_ID}" -ojsonpath='{.metadata.labels.ceph\.rook\.io/pvc}')
pv_name=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pvc \
  "${pvc_name}" -o jsonpath='{.spec.volumeName}')
----

. Check if the OSD deployment needs to be deleted, and delete it if necessary
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy \
  -l failure-domain="${NODE_TO_REMOVE}"
# Run this command if the previous command lists a deployment
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete deploy \
  -l failure-domain="${NODE_TO_REMOVE}"
----

. Clean up PVC and prepare job of the old OSD if necessary
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete job \
  -l ceph.rook.io/pvc="${pvc_name}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete pvc "${pvc_name}"
----

. Clean up PVC encryption secret
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete secret -l pvc_name="${pvc_name}"
----

=== Remove the old MON

. Find the MON (if any) on the node to be replaced
+
[source,bash]
----
MON_ID=$(kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods \
  -lapp=rook-ceph-mon -o wide \
  | grep "${NODE_TO_REMOVE}" | cut -d- -f4)
echo $MON_ID
----
+
TIP: You can skip the remaining steps in this section if `$MON_ID` is empty.

. Disable auto sync for component `rook-ceph`.
This allows us to temporarily make manual changes to the Rook Ceph cluster.
+
include::partial$disable-argocd-autosync.adoc[]

. Temporarily adjust the Rook MON failover timeout.
This tells the operator to perform the MON failover after less time than the default 10 minutes.
+
[NOTE]
====
We currently have to restart the operator to force it to pick up the new MON health check configuration.
Once https://github.com/rook/rook/issues/8363[Rook.io GitHub issue #8363] is fixed, the operator restart shouldn't be necessary anymore.
====
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p '[{
    "op": "replace",
    "path": "/spec/healthCheck/daemonHealth/mon",
    "value": {
      "disabled": false,
      "interval": "10s",
      "timeout": "10s"
    }
  }]'
kubectl --as=cluster-admin -n syn-rook-ceph-operator delete pods \
  -l app=rook-ceph-operator
----

. Wait for operator to settle.
Wait for a log message saying `done reconciling ceph cluster in namespace "syn-rook-ceph-cluster"`
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-operator logs -f \
  deploy/rook-ceph-operator
----

. Cordon node to replace and delete MON pod
+
[source,bash]
----
kubectl --as=cluster-admin cordon "${NODE_TO_REMOVE}"
kubectl --as=cluster-admin -n syn-rook-ceph-cluster delete pod \
  -l app=rook-ceph-mon,ceph_daemon_id="${MON_ID}"
----

. Wait until new MON is scheduled
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get pods -w
----
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster exec -it deploy/rook-ceph-tools -- \
  ceph status
----

. Verify that three MONs are running
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster get deploy -l app=rook-ceph-mon
----

. Reset the MON failover timeout
+
[NOTE]
====
We currently have to restart the operator to force it to pick up the new MON health check configuration.
Once https://github.com/rook/rook/issues/8363[Rook.io GitHub issue #8363] is fixed, the operator restart shouldn't be necessary anymore.
====
+
[source,bash]
----
kubectl --as=cluster-admin -n syn-rook-ceph-cluster patch cephcluster cluster --type=json \
  -p '[{
    "op": "replace",
    "path": "/spec/healthCheck/daemonHealth/mon",
    "value": {}
  }]'
kubectl --as=cluster-admin -n syn-rook-ceph-operator delete pods \
  -l app=rook-ceph-operator
----

include::partial$enable-argocd-autosync.adoc[]

=== Remove VM


. Find Terraform resource index of the node to replace
+
[source,bash]
----
# Grab JSON copy of current Terraform state
terraform state pull > .tfstate.json
node_index=$(jq --arg storage_node "${NODE_TO_REMOVE}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="random_id") |
   .instances[] |
   select(.attributes.hex==$storage_node) |
   .index_key' \
  .tfstate.json)
----

. Verify that resource index is correct
+
[source,bash]
----
jq --arg index "${node_index}" -r \
  '.resources[] |
   select(.module=="module.cluster.module.storage" and .type=="exoscale_compute") |
   .instances[$index|tonumber] |
   .attributes.hostname' \
   .tfstate.json
----

. Remove node ID and node resource for node that we want to remove from the Terraform state
+
[source,bash]
----
terraform state rm "module.cluster.module.storage.random_id.node_id[$node_index]"
terraform state rm "module.cluster.module.storage.exoscale_compute.nodes[$node_index]"
----

. Drain the node to replace
+
[source,bash]
----
kubectl --as=cluster-admin drain "${NODE_TO_REMOVE}" \
  --delete-emptydir-data --ignore-daemonsets
----

. Delete the node to replace from the cluster
+
[source,bash]
----
kubectl --as=cluster-admin delete node "${NODE_TO_REMOVE}"
----

. Find the Exoscale node id of the node to replace
+
[source,bash]
----
node_id=$(exo vm list -O json | \
  jq --arg storage_node "$NODE_TO_REMOVE" -r \
  '.[] | select(.name==$storage_node) | .id')
----

. Verify that the node ID is correct
+
[source,bash]
----
exo vm list | grep "${node_id}"
----

. Delete the node
+
[source,bash]
----
exo vm delete "${node_id}"
----

. Clean up localstorage PV of decommissioned node
+
[source,bash]
----
kubectl --as=cluster-admin delete pv "${pv_name}"
----

== Finish up

include::partial$remove-alertmanager-silence.adoc[]


== Upstream documentation

* Rook documentation
** https://rook.io/docs/rook/v1.6/ceph-osd-mgmt.html#remove-an-osd[Remove an OSD]
** https://rook.io/docs/rook/v1.6/ceph-mon-health.html#failing-over-a-monitor[MON failover]
