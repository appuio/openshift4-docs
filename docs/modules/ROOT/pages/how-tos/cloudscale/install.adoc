= Installation on cloudscale.ch

[abstract]
--
Steps to install an OpenShift 4 cluster on https://cloudscale.ch[cloudscale.ch].

These steps follow the https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/installing-bare-metal.html[Installing a cluster on bare metal] docs to set up a user provisioned installation (UPI).
https://www.terraform.io[Terraform] is used to provision the cloud infrastructure.
--

[NOTE]
--
The commands are idempotent and can be retried if any of the steps fail.

The certificates created during bootstrap are only valid for 24h.
So make sure you complete these steps within 24h.
--

[NOTE]
--
This how-to guide is still a work in progress and will change.
It's currently very specific to VSHN and needs further changes to be more generic.
--


== Prerequisites

* `docker`
* `mc` https://docs.min.io/docs/minio-client-quickstart-guide.html[Minio client] (aliased to `mc` if necessary)
* `jq`
* `yq` https://mikefarah.gitbook.io/yq[yq YAML processor]
* `openshift-install` https://cloud.redhat.com/openshift/install/metal/user-provisioned[OpenShift Installer]


== Cluster Installation

Register the new OpenShift 4 cluster in Lieutenant.

.Lietenant API endpoint
****
Use the following endpoint for Lieutenant:

VSHN:: https://control.vshn.net/syn/lieutenantclusters
****

=== Configure input

[source,console]
----
export CLOUDSCALE_TOKEN=<cloudscale-api-token> # From https://control.cloudscale.ch/user/api-tokens
export GITLAB_TOKEN=<gitlab-api-token> # From https://git.vshn.net/profile/personal_access_tokens
export GITLAB_CATALOG_PROJECT_ID=<project-id> # GitLab numerical project ID of the catalog repo
export REGION=rma # rma or lpg (without the zone number)
export CLUSTER_ID=<lieutenant-cluster-id>
export TENANT_ID=<lieutenant-tenant-id>
export BASE_DOMAIN=appuio-beta.ch # See xref:explanation/dns_scheme.adoc[DNS Scheme]
export PULL_SECRET=<redhat-pull-secret> # As copied from https://cloud.redhat.com/openshift/install/pull-secret "Copy pull secret"
export COMMODORE_API_URL=<lieutenant-api-endpoint> # For example: https://api-int.syn.vshn.net
export COMMODORE_GLOBAL_GIT_BASE=ssh://git@git.vshn.net/syn
export COMMODORE_API_TOKEN=<lieutenant-api-token> # See https://wiki.vshn.net/pages/viewpage.action?pageId=167838622#ClusterRegistryinLieutenantSynfectaCluster(synfection)-Preparation
----

=== Set up S3 bucket

. Create S3 bucket

.. If a bucket user already exists for this cluster:
+
[source,console]
----
# Use already exiting bucket user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  https://api.cloudscale.ch/v1/objects-users | \
  jq -e ".[] | select(.display_name == \"${CLUSTER_ID}\")")
----

.. To create a new bucket user:
+
[source,console]
----
# Create a new user
response=$(curl -sH "Authorization: Bearer ${CLOUDSCALE_TOKEN}" \
  -F display_name=${CLUSTER_ID} \
  https://api.cloudscale.ch/v1/objects-users)
----

. Configure the Minio client
+
[source,console]
----
mc config host add \
  "${CLUSTER_ID}" "https://objects.${REGION}.cloudscale.ch" \
  $(echo $response | jq -r '.keys[0].access_key') \
  $(echo $response | jq -r '.keys[0].secret_key')

mc mb --ignore-existing \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition"
----

=== OpenShift Installer Setup

For the following steps, change into a clean directory (for example a /tmp/ directory).

[CAUTION]
These are the only steps which aren't idempotent.
If you have to recreate the install config or any of the generated manifests you need to rerun all of the subsequent steps.

. Prepare `install-config.yaml`
+
[source,console]
----
mkdir -p target

cat > target/install-config.yaml <<EOF
apiVersion: v1
metadata:
  name: ${CLUSTER_ID}
baseDomain: ${BASE_DOMAIN}
platform:
  none: {}
pullSecret: |
  ${PULL_SECRET}
EOF
----

. Render install manifests (this will consume the `install-config.yaml`)
+
[source,console]
----
openshift-install --dir target \
  create manifests
----

.. If you want to change the default "apps" domain for the cluster:
+
[source,console]
----
yq w -i target/manifests/cluster-ingress-02-config.yml \
  spec.domain apps.example.com
----

. Render and upload ignition config (this will consume all the manifests)
+
[source,console]
----
openshift-install --dir target \
  create ignition-configs

mc cp target/bootstrap.ign "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/"

export TF_VAR_ignition_bootstrap=$(mc share download \
  --json --expire=1h \
  "${CLUSTER_ID}/${CLUSTER_ID}-bootstrap-ignition/bootstrap.ign" | jq -r '.share')
----

=== Terraform Cluster Config

. Configure Commodore
+
[source,console]
----
docker pull docker.io/projectsyn/commodore:latest

commodore () {
    mkdir -p inventory/classes/global dependencies/lib compiled/ catalog/
    docker run \
    --interactive=true \
    --tty \
    --rm \
    --user="$(id -u):$(id -u)" \
    --volume "$HOME"/.ssh:/app/.ssh:ro \
    --volume "$PWD"/compiled/:/app/compiled/ \
    --volume "$PWD"/catalog/:/app/catalog \
    --volume "$PWD"/dependencies/:/app/dependencies/ \
    --volume "$PWD"/inventory/:/app/inventory/ \
    --volume ~/.gitconfig:/app/.gitconfig:ro \
    -e COMMODORE_API_URL=$COMMODORE_API_URL \
    -e COMMODORE_GLOBAL_GIT_BASE=$COMMODORE_GLOBAL_GIT_BASE \
    -e COMMODORE_API_TOKEN=$COMMODORE_API_TOKEN \
    projectsyn/commodore:latest \
    $*
}
----

. Prepare Commodore inventory.
+
This command will fail due to circular dependencies in the Commodore setup.
You will see error messages starting with `Cannot resolve ${openshift:*}`.
As long as all components are cloned for the cluster it's enough to proceed.
+
This can be improved once https://github.com/projectsyn/commodore/issues/135[this issue] is solved.
+
[source,console]
----
# This will fail
commodore catalog compile ${CLUSTER_ID}
----

. Prepare Terraform cluster config
+
[source,console]
----
CA_CERT=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
  target/master.ign | \
  awk -F ',' '{ print $2 }' | \
  base64 --decode)

pushd "inventory/classes/${TENANT_ID}/"

yq w -i "${CLUSTER_ID}.yml" \
  "classes[+]" "components.openshift4-cloudscale"

yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.infraID -- "$(jq -r .infraID ../../../target/metadata.json)"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.clusterID -- "$(jq -r .clusterID ../../../target/metadata.json)"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift.appsDomain -- "apps.${CLUSTER_ID}.${BASE_DOMAIN}"

yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift4_cloudscale.variables.base_domain -- "${BASE_DOMAIN}"
yq w -i "${CLUSTER_ID}.yml" \
  parameters.openshift4_cloudscale.variables.ignition_ca -- "${CA_CERT}"

git commit -a -m "Setup cluster ${CLUSTER_ID}"
git push

popd
----

. Compile and push Terraform setup
+
[source,console]
----
commodore catalog compile ${CLUSTER_ID} --push -i
----

=== Provision Infrastructure

. Setup Terraform
+
[source,console]
----
tf_image=$(\
  yq r dependencies/openshift4-cloudscale/class/defaults.yml \
  parameters.openshift4_cloudscale.images.terraform.image)
tf_tag=$(\
  yq r dependencies/openshift4-cloudscale/class/defaults.yml \
  parameters.openshift4_cloudscale.images.terraform.tag)

alias terraform='docker run -it --rm \
  -e CLOUDSCALE_TOKEN="${CLOUDSCALE_TOKEN}" \
  -e TF_VAR_ignition_bootstrap="${TF_VAR_ignition_bootstrap}" \
  -w /tf \
  -v $(pwd):/tf \
  -v $CLUSTER_ID:/tf/.terraform \
  --ulimit memlock=-1 \
  ${tf_image}:${tf_tag} terraform'

export GITLAB_STATE_URL="https://git.vshn.net/api/v4/projects/${GITLAB_CATALOG_PROJECT_ID}/terraform/state/cluster"

pushd catalog/manifests/openshift4-cloudscale/

terraform init \
  "-backend-config=address=${GITLAB_STATE_URL}" \
  "-backend-config=lock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=unlock_address=${GITLAB_STATE_URL}/lock" \
  "-backend-config=username=$(whoami)" \
  "-backend-config=password=${GITLAB_TOKEN}" \
  "-backend-config=lock_method=POST" \
  "-backend-config=unlock_method=DELETE" \
  "-backend-config=retry_wait_min=5"
----

. Provision bootstrap node
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  master_count    = 0
  infra_count     = 0
  worker_count    = 0
}
EOF

terraform apply
----

. Create the shown DNS records

. Wait for the DNS records to propagate!
+
[source,console]
----
sleep 600
host "api.${CLUSTER_ID}.${BASE_DOMAIN}"
----

. Provision master nodes
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  bootstrap_count = 1
  infra_count     = 0
  worker_count    = 0
}
EOF

terraform apply
----

. Create the remaining DNS records
+
[source,console]
----
terraform output -json | jq -r ".cluster_dns.value"
----

. Wait for bootstrap to complete
+
[source,console]
----
openshift-install --dir ../../../target \
  wait-for bootstrap-complete
----

. Remove bootstrap node and provision infra nodes
+
[source,console]
----
cat > override.tf <<EOF
module "cluster" {
  worker_count    = 0
}
EOF

terraform apply

export KUBECONFIG="$(pwd)/../../../target/auth/kubeconfig"

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
while sleep 3; do \
  oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve; \
done

kubectl get nodes -lnode-role.kubernetes.io/worker
kubectl label node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/infra=""
----

. Wait for installation to complete
+
[source,console]
----
openshift-install --dir ../../../target \
  wait-for install-complete
----

. Provision worker nodes
+
[source,console]
----
rm override.tf

terraform apply

# Once CSRs in state Pending show up, approve them
# Needs to be run twice, two CSRs for each node need to be approved
while sleep 3; do \
  oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | \
  xargs oc adm certificate approve; \
done

kubectl label --overwrite node -lnode-role.kubernetes.io/worker \
  node-role.kubernetes.io/app=""
kubectl label node -lnode-role.kubernetes.io/infra \
  node-role.kubernetes.io/app-
----

. Create secret with S3 credentials https://docs.openshift.com/container-platform/4.5/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html#registry-operator-config-resources-secret-aws_configuring-registry-storage-aws-user-infrastructure[for the registry] (will be https://ticket.vshn.net/browse/APPU-2790[automated])
+
[source,console]
----
oc create secret generic image-registry-private-configuration-user \
--namespace openshift-image-registry \
--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=$(mc config host ls ${CLUSTER_ID} -json | jq -r .accessKey) \
--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=$(mc config host ls ${CLUSTER_ID} -json | jq -r .secretKey)
----

. Make the cluster Project Syn enabled
+
Install Steward on the cluster according to https://wiki.vshn.net/x/ngMBCg

. Save the admin credentials in the https://password.vshn.net[password manager].
You can find the password in the file `target/auth/kubeadmin-password` and the kubeconfig in `target/auth/kubeconfig`
+
[source,console]
----
popd
ls -l target/auth/
----

. Delete local config files
+
[source,console]
----
rm -r target/
----
